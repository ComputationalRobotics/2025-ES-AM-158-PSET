{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d7c41cb8",
   "metadata": {},
   "source": [
    "# Problem Set 1 — Four Problems in Dynamic Programming\n",
    "\n",
    "*Markov Decision Processes & DP Methods*\n",
    "\n",
    "**Due: September 26 (Friday), 11:59pm**\n",
    "\n",
    "### Problem 1 — Inscribed Polygon of Maximum Perimeter (Pen & Paper)\n",
    "TODO list:\n",
    "- (a) show Q-value function $Q_{N-1}$ (6pt)\n",
    "- (b) show convexity (6pt)\n",
    "- (c) show optimal control signal $u_{N-1}$ (6pt)\n",
    "- (d) induction to any $k$-th step Q-function $Q_k$ (6pt)\n",
    "- (e) show all optimal control signal $u_k$ (6pt)\n",
    "\n",
    "Bonus:\n",
    "- (f) show convexity (5pt)\n",
    "- (g)(coding) solve the problem using optimization (5pt)\n",
    "### Problem 2 — Proof of convergence of value iteration (Pen & Paper)\n",
    "TODO list:\n",
    "- 2.1 contraction of bellman operator (5pt)\n",
    "- 2.2 linear convergence (5pt)\n",
    "- 2.3 stoping criteria (5pt)\n",
    "- 2.4 iteration bound (5pt)\n",
    "### Problem 3 — Cliffwalk (coding)\n",
    "TODO list:\n",
    "- 3.2 fill in code for policy evaluation (10pt)\n",
    "- 3.3 fill in code for policy iteration (10pt)\n",
    "- 3.4 fill in code for value iteration (10pt)\n",
    "\n",
    "### Problem 4 — Matrix–Vector Representation of DP\n",
    "TODO list:\n",
    "- 4.1 build the transition matrix $P$ (5pt)\n",
    "- 4.2 write bellman equation as matrix form (5pt)\n",
    "- 4.3 solve the matrix equation by fix-point iteration (10pt)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "999f44b8",
   "metadata": {},
   "source": [
    "## 1. Inscribed Polygon of Maximal Perimeter (Pen and Paper)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bccd1ea",
   "metadata": {},
   "source": [
    "In lectures, we have seen how dynamic programming (DP) can compute optimal value functions and optimal policies for finite-horizon MDPs with discrete state space and action space (i.e., the tabular case).\n",
    "\n",
    "In this exercise, we will see that DP can also solve an optimal control problem with continuous state space and action space.\n",
    "This problem is a geometry problem where we try to find the $N$-side polygon inscribed inside a circle with maximum perimeter. We will walk you through the key steps of formulating and solving the problem, while leaving a few mathematical details for you to fill in.\n",
    "\n",
    "Given a circle with radius $1$, we can randomly choose $N$ distinct points on the circle to form a polygon with $N$ vertices and sides, as shown in Fig. 1 with $N=3,4,5$.\n",
    "\n",
    "<figure style=\"text-align:center;\">\n",
    "  <img src=\"https://raw.githubusercontent.com/ComputationalRobotics/2025-ES-AM-158-PSET/main/PSET1/polygon-inside-circle.png\" width=\"600\" alt=\"Inscribed polygon\">\n",
    "  <figcaption style=\"color:#6a737d; font-style:italic;\">\n",
    "    Figure 1. Polygons inscribed inside a circle\n",
    "  </figcaption>\n",
    "</figure>\n",
    "\n",
    "Once the $N$ points are chosen, the $N$-polygon will have a perimeter, i.e., the sum of the lengths of its edges.\n",
    "\n",
    "What is the configuration of the $N$ points such that the resulting $N$-polygon has the maximum perimeter? We claim that the answer is when the $N$-polygon has edges of equal lengths, or in other words, when the $N$ points are placed on the circle evenly.\n",
    "\n",
    "Let us use dynamic programming to prove the claim.\n",
    "\n",
    "To use dynamic programming, we need to define a dynamical system and a reward function.\n",
    "\n",
    "<figure style=\"text-align:center;\">\n",
    "  <img src=\"https://raw.githubusercontent.com/ComputationalRobotics/2025-ES-AM-158-PSET/main/PSET1/sequential-placement-N-point.png\" width=\"360\" alt=\"Inscribed polygon\">\n",
    "  <figcaption style=\"color:#6a737d; font-style:italic;\">\n",
    "    Figure 2. Sequential placement of N points on the circle.\n",
    "  </figcaption>\n",
    "</figure>\n",
    "\n",
    "**Dynamical system.**\n",
    "\n",
    "\n",
    "We will use $\\{x_1, \\ldots, x_N\\}$ to denote the angular positions of the $N$ points to be placed on the circle (with slight abuse of notation, we will call each of those points $x_k$ as well). In particular, as shown in Fig. 2, let us use $x_k$ to denote the angle between the line $O — x_k$ and the vertical line (O is the center of the circle), with zero angle starting at 12 o’clock and clockwise being positive. Without loss of generality, we assume $x_1 = 0$. (if $x_1$ is nonzero, we can always rotate the entire circle so that $x_1 = 0$).\n",
    "\n",
    "After the $k$-th point is placed, we can “control” where the next point $x_{k+1}$ will be, by deciding the incremental angle between $x_{k+1}$ and $x_k$, denoted as $u_k > 0$ in Fig. 2. This is simply saying the dynamics is\n",
    "\n",
    "$$\n",
    "x_{k+1} = x_k + u_k, \\quad k=1,\\ldots,N-1, \\quad x_1 = 0.\n",
    "$$\n",
    "\n",
    "Notice here we did not use an MDP to formulate this problem because the dynamics is deterministic. In the MDP language, this would correspond to, at time step $k$, if the agent takes action $u_k$ at state $x_k$, then the probability of transitioning to state $x_k + u_k$ at time $k+1$ is $1$, and the probability of transitioning to other states is zero.\n",
    "\n",
    "\n",
    "**Reward.**\n",
    "\n",
    "\n",
    "The perimeter of the $N$-polygon is therefore\n",
    "\n",
    "$$\n",
    "g_N(x_N) + \\sum_{k=1}^{N-1} g_k(x_k,u_k),\n",
    "$$\n",
    "\n",
    "with the terminal reward\n",
    "\n",
    "$$\n",
    "g_N(x_N) = 2 \\sin\\left(\\frac{2\\pi-x_N}{2}\\right),\n",
    "$$\n",
    "\n",
    "the distance between $x_N$ and $x_1$ (see Fig. 2), and the running reward\n",
    "\n",
    "$$\n",
    "g_k(x_k,u_k) = 2 \\sin\\left(\\frac{u_k}{2}\\right).\n",
    "$$\n",
    "\n",
    "**Dynamic programming.**\n",
    "\n",
    "We are now ready to invoke dynamic programming. Recall in lectures the key steps of DP are first to initialize the optimal value functions at the terminal time $k=N$, and then perform backward recursion to compute the optimal value functions at time $k=N-1,\\dots,1$.\n",
    "\n",
    "We start by setting\n",
    "\n",
    "$$\n",
    "V_N(x_N) = g_N(x_N) = 2 \\sin\\left(\\frac{2\\pi-x_N}{2}\\right).\n",
    "$$\n",
    "\n",
    "Unlike in lectures where we initialized the terminal value functions as all zero, here we initialize the terminal value functions as $g_N(x_N)$ because there is a \"terminal-state\" reward.\n",
    "\n",
    "We then compute $V_{N-1}(x_{N-1})$ as\n",
    "\n",
    "$$\n",
    "V_{N-1}(x_{N-1})\n",
    "= \\max_{0 < u_{N-1} < 2\\pi-x_{N-1}}\n",
    "\\left\\{\n",
    "  \\underbrace{ 2 \\sin\\left(\\tfrac{u_{N-1}}{2}\\right) + V_N(x_{N-1} + u_{N-1}) }_{Q_{N-1}(x_{N-1}, u_{N-1})}\n",
    "\\right\\},\n",
    "\\tag{9.1}\n",
    "$$\n",
    "\n",
    "where $u_{N-1} < 2\\pi-x_{N-1}$ because we do not want $x_N$ to cross $2\\pi$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c0764d9",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "**a**. Show that\n",
    "\n",
    "$$\n",
    "Q_{N-1}(x_{N-1}, u_{N-1})\n",
    "= 2 \\sin\\left(\\tfrac{u_{N-1}}{2}\\right)\n",
    "+ 2 \\sin\\left(\\tfrac{2\\pi-x_{N-1}-u_{N-1}}{2}\\right),\n",
    "$$\n",
    "\n",
    "and thus\n",
    "\n",
    "$$\n",
    "\\frac{\\partial Q_{N-1}(x_{N-1}, u_{N-1})}{\\partial u_{N-1}}\n",
    "= \\cos\\left(\\tfrac{u_{N-1}}{2}\\right)\n",
    "- \\cos\\left(\\tfrac{2\\pi-x_{N-1}-u_{N-1}}{2}\\right).\n",
    "$$\n",
    "\n",
    "**(TODO) ANSWER:**\n"
   ]
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "---\n",
    "**Answer: 1.a**\n",
    "\n",
    "From class, we derived that the Bellman optimality equation for $Q^*$ is\n",
    "$$\n",
    "Q^*(x_{N-1}, u_{N-1}) = \\sum_{x_{N}, r} P(x_{N}, r \\mid x_{N-1}, u_{N-1})\\,\\Big[ r + \\gamma \\max_{u_{N}} Q^*(x_{N}, u_{N}) \\Big],\n",
    "$$\n",
    "where $r$ is the immediate reward.\n",
    "\n",
    "Intuitively, this says that the optimal action-value of $(x_{N-1}, u_{N-1})$, the **penultimate state**, is the immediate reward plus the discounted value of the best action in the final step.\n",
    "\n",
    "Within the context of this problem, we know that the transition dynamics are deterministic. Likewise, we can set the discount factor $\\gamma = 1$, as this is given to be a finite-horizon MDP. Thus, we can simplify:\n",
    "\n",
    "$$\n",
    "Q^*(x_{N-1}, u_{N-1}) = \\sum_{x_{N}, r} P(x_{N}, r \\mid x_{N-1}, u_{N-1})\\,\\Big[ r + \\max_{u_{N}} Q^*(x_{N}, u_{N}) \\Big],\n",
    "$$\n",
    "\n",
    "so that\n",
    "$$\n",
    "Q^*(s, a) = r + \\max_{u_{N}} Q^*(x_{N}, u_{N}).\n",
    "$$\n",
    "\n",
    "In addition, we are given the reward, which is the additional length of every $u_{N-1}$:\n",
    "\n",
    "$$\n",
    "r = 2 \\sin\\!\\left(\\tfrac{u_{N-1}}{2}\\right).\n",
    "$$\n",
    "\n",
    "Now, we need to make sense of $\\max_{u_{N}} Q^*(x_{N}, u_{N})$, which is the optimal value of the **final state** $u_{N}$. At this last step, we are bounded between $x_{N-1} + u_{N-1}$ and $2\\pi$. This means the reward of the final step is\n",
    "\n",
    "$$\n",
    "\\max_{u_{N}} Q^*(x_{N}, u_{N}) = 2 \\sin\\!\\left(\\tfrac{2\\pi - x_{N-1} - u_{N-1}}{2}\\right).\n",
    "$$\n",
    "\n",
    "Putting it all together, we obtain\n",
    "\n",
    "$$\n",
    "Q^*(x_{N-1}, u_{N-1}) = 2 \\sin\\!\\left(\\tfrac{u_{N-1}}{2}\\right) + 2 \\sin\\!\\left(\\tfrac{2\\pi - x_{N-1} - u_{N-1}}{2}\\right).\n",
    "$$\n",
    "\n",
    "When we take the partial derivative of $Q^*(x_{N-1}, u_{N-1})$ with respect to $u_{N-1}$, we arrive at the following:\n",
    "\n",
    "$$\n",
    "\\frac{\\partial Q^*}{\\partial u_{N-1}}\n",
    "= 2\\cos\\!\\Big(\\tfrac{u_{N-1}}{2}\\Big)\\cdot\\tfrac{1}{2}\n",
    "+ 2\\cos\\!\\Big(\\tfrac{2\\pi - x_{N-1} - u_{N-1}}{2}\\Big)\\cdot\\frac{\\partial}{\\partial u_{N-1}}\\!\\Big(\\tfrac{2\\pi - x_{N-1} - u_{N-1}}{2}\\Big),\n",
    "$$\n",
    "\n",
    "which simplifies to\n",
    "\n",
    "$$\n",
    "\\frac{\\partial Q^*}{\\partial u_{N-1}}\n",
    "=\\cos\\!\\Big(\\tfrac{u_{N-1}}{2}\\Big)\\;-\\;\\cos\\!\\Big(\\tfrac{2\\pi - x_{N-1} - u_{N-1}}{2}\\Big).\n",
    "$$\n",
    "\n",
    "---"
   ],
   "id": "4d186d7f967a3448"
  },
  {
   "cell_type": "markdown",
   "id": "a666ead0",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "**b**. Show that $Q_{N-1}(x_{N-1}, u_{N-1})$ is concave (i.e., $-Q_{N-1}(x_{N-1}, u_{N-1})$ is convex) in $u_{N-1}$ for every $x_{N-1} \\in (0, \\pi)$ and $u_{N-1} \\in (0, 2\\pi-x_{N-1})$.\n",
    "(Hint: compute the second derivative of $Q_{N-1}(x_{N-1}, u_{N-1})$ with respect to $u_{N-1}$ and verify it is positive definite)\n",
    "\n",
    "**(TODO) ANSWER:**"
   ]
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "---\n",
    "**Answer: 1.b**\n",
    "\n",
    "We start with\n",
    "$$\n",
    "Q_{N-1}(x_{N-1}, u_{N-1})\n",
    "= 2 \\sin\\!\\left(\\tfrac{u_{N-1}}{2}\\right)\n",
    "+ 2 \\sin\\!\\left(\\tfrac{2\\pi - x_{N-1} - u_{N-1}}{2}\\right).\n",
    "$$\n",
    "\n",
    "Taking the first derivative with respect to $u_{N-1}$:\n",
    "$$\n",
    "\\frac{\\partial Q_{N-1}}{\\partial u_{N-1}}\n",
    "= \\cos\\!\\Big(\\tfrac{u_{N-1}}{2}\\Big)\n",
    "- \\cos\\!\\Big(\\tfrac{2\\pi - x_{N-1} - u_{N-1}}{2}\\Big).\n",
    "$$\n",
    "\n",
    "Taking the second derivative:\n",
    "$$\n",
    "\\frac{\\partial^2 Q_{N-1}}{\\partial u_{N-1}^2}\n",
    "= -\\tfrac{1}{2}\\sin\\!\\Big(\\tfrac{u_{N-1}}{2}\\Big)\n",
    "- \\tfrac{1}{2}\\sin\\!\\Big(\\tfrac{2\\pi - x_{N-1} - u_{N-1}}{2}\\Big).\n",
    "$$\n",
    "\n",
    "Now note that on the domain\n",
    "$$\n",
    "x_{N-1} \\in (0,\\pi), \\qquad u_{N-1} \\in (0,\\,2\\pi - x_{N-1}),\n",
    "$$\n",
    "we have\n",
    "$$\n",
    "\\tfrac{u_{N-1}}{2} \\in (0,\\pi),\n",
    "\\qquad \\tfrac{2\\pi - x_{N-1} - u_{N-1}}{2} \\in (0,\\pi).\n",
    "$$\n",
    "Thus both sine terms are strictly positive, implying\n",
    "$$\n",
    "\\frac{\\partial^2 Q_{N-1}}{\\partial u_{N-1}^2} < 0\n",
    "\\quad\\text{throughout the domain}.\n",
    "$$\n",
    "\n",
    "Therefore, $Q_{N-1}(x_{N-1}, u_{N-1})$ is **strictly concave** in $u_{N-1}$ for all $x_{N-1} \\in (0,\\pi)$ and $u_{N-1} \\in (0,\\,2\\pi - x_{N-1})$.\n",
    "\n",
    "---"
   ],
   "id": "7f39da57637774a4"
  },
  {
   "cell_type": "markdown",
   "id": "a71bd6ca",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "**c**. With a and b, show that the optimal $u_{N-1}$ that solves (9.1) is\n",
    "\n",
    "$$\n",
    "u_{N-1}^\\star = \\frac{2\\pi-x_{N-1}}{2},\n",
    "$$\n",
    "\n",
    "and therefore\n",
    "\n",
    "$$\n",
    "J_{N-1}(x_{N-1}) = 4 \\sin\\left(\\tfrac{2\\pi-x_{N-1}}{4}\\right).\n",
    "$$\n",
    "\n",
    "(Hint: the point at which a concave function’s gradient vanishes must be the unique maximizer of that function.)\n",
    "\n",
    "**(TODO) ANSWER:**"
   ]
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "---\n",
    "**Answer: 1.c**\n",
    "\n",
    "Because $Q^*$ is strictly concave, any stationary point is the **unique** maximizer. Setting the first derivative to zero,\n",
    "$$\n",
    "\\cos\\!\\Big(\\tfrac{u_{N-1}}{2}\\Big)\n",
    "=\\cos\\!\\Big(\\tfrac{2\\pi - x_{N-1} - u_{N-1}}{2}\\Big)\n",
    "\\;\\Rightarrow\\;\n",
    "u_{N-1}^*=\\frac{2\\pi - x_{N-1}}{2},\n",
    "$$\n",
    "which lies in $\\big(0,\\,2\\pi-x_{N-1}\\big)$. Thus, the unique optimal split at step $N-1$ is to take **half of the remaining angle**; the final step $u_N^*$ takes the other half.\n",
    "\n",
    "Knowing this, we can plug back into our state-action value function to find $J_{N-1}(x_{N-1})$:\n",
    "\n",
    "$$\n",
    "J_{N-1}(x_{N-1}) = Q_{N-1}(x_{N-1}, u_{N-1}^*) = 4 \\sin\\!\\Big(\\tfrac{2\\pi - x_{N-1}}{4}\\Big).\n",
    "$$\n",
    "\n",
    "---"
   ],
   "id": "ade94876d90259b7"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "\n",
    "\n",
    "**d**. Now use induction to show that the $k$-th step dynamic programming\n",
    "\n",
    "$$\n",
    "J_k(x_k)\n",
    "= \\max_{0 < u_k < 2\\pi — x_k}\n",
    "\\left\\{ 2 \\sin\\left(\\tfrac{u_k}{2}\\right) + J_{k+1}(x_k + u_k) \\right\\}\n",
    "$$\n",
    "\n",
    "admits an optimal control\n",
    "\n",
    "$$\n",
    "u_k^\\star = \\frac{2\\pi-x_k}{N-k + 1},\n",
    "$$\n",
    "\n",
    "and optimal cost-to-go\n",
    "\n",
    "$$\n",
    "J_k(x_k) = 2 (N-k + 1) \\, \\sin\\!\\left( \\frac{2\\pi-x_k}{2 (N-k + 1)} \\right).\n",
    "$$\n",
    "\n",
    "**(TODO) ANSWER:**"
   ],
   "id": "463c9458"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "---\n",
    "**Answer: 1.d**\n",
    "\n",
    "We prove the claim by induction on the step index $k$.\n",
    "\n",
    "**Base Case ($k = N-1$).**\n",
    "From part (c), we showed that\n",
    "$$\n",
    "u_{N-1}^\\star = \\frac{2\\pi - x_{N-1}}{2}, \\qquad\n",
    "J_{N-1}(x_{N-1}) = 4 \\sin\\!\\left(\\frac{2\\pi - x_{N-1}}{4}\\right).\n",
    "$$\n",
    "This matches the proposed formula with $N-k+1 = 2$:\n",
    "$$\n",
    "u_{N-1}^\\star = \\frac{2\\pi - x_{N-1}}{2}, \\qquad\n",
    "J_{N-1}(x_{N-1}) = 2 (2) \\sin\\!\\left(\\frac{2\\pi - x_{N-1}}{2 \\cdot 2}\\right).\n",
    "$$\n",
    "So the base case holds.\n",
    "\n",
    "**Inductive Step.**\n",
    "Assume the formula holds for step $k+1$:\n",
    "$$\n",
    "u_{k+1}^\\star = \\frac{2\\pi - x_{k+1}}{N-k}, \\qquad\n",
    "J_{k+1}(x_{k+1}) = 2 (N-k)\\,\\sin\\!\\left(\\frac{2\\pi - x_{k+1}}{2 (N-k)}\\right).\n",
    "$$\n",
    "\n",
    "We need to show it also holds for step $k$.\n",
    "\n",
    "By definition of dynamic programming,\n",
    "$$\n",
    "J_k(x_k) = \\max_{0 < u_k < 2\\pi - x_k}\n",
    "\\left\\{ 2 \\sin\\!\\left(\\tfrac{u_k}{2}\\right) + J_{k+1}(x_k + u_k) \\right\\}.\n",
    "$$\n",
    "\n",
    "Substitute the inductive hypothesis for $J_{k+1}$:\n",
    "$$\n",
    "J_k(x_k) = \\max_{0 < u_k < 2\\pi - x_k}\n",
    "\\left\\{ 2 \\sin\\!\\left(\\tfrac{u_k}{2}\\right)\n",
    "+ 2 (N-k)\\,\\sin\\!\\Bigg(\\frac{2\\pi - (x_k + u_k)}{2 (N-k)}\\Bigg) \\right\\}.\n",
    "$$\n",
    "\n",
    "**Concavity Argument.**\n",
    "The objective inside the braces is strictly concave in $u_k$, so the maximizer is unique.\n",
    "Setting the first derivative with respect to $u_k$ equal to zero yields\n",
    "$$\n",
    "\\cos\\!\\left(\\tfrac{u_k}{2}\\right)\n",
    "= \\cos\\!\\left(\\frac{2\\pi - x_k - u_k}{2 (N-k)}\\right).\n",
    "$$\n",
    "The feasible solution is\n",
    "$$\n",
    "u_k^\\star = \\frac{2\\pi - x_k}{N-k+1}.\n",
    "$$\n",
    "\n",
    "**Compute the optimal cost.**\n",
    "Plugging $u_k^\\star$ back into the expression for $J_k$:\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "J_k(x_k) &= 2 \\sin\\!\\left(\\frac{u_k^\\star}{2}\\right)\n",
    "+ 2 (N-k)\\,\\sin\\!\\left(\\frac{2\\pi - x_k - u_k^\\star}{2 (N-k)}\\right).\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "Using $u_k^\\star = \\tfrac{2\\pi - x_k}{N-k+1}$, both sine arguments simplify to the same value:\n",
    "$$\n",
    "\\frac{u_k^\\star}{2} = \\frac{2\\pi - x_k}{2 (N-k+1)},\n",
    "\\qquad\n",
    "\\frac{2\\pi - x_k - u_k^\\star}{2 (N-k)}\n",
    "= \\frac{2\\pi - x_k}{2 (N-k+1)}.\n",
    "$$\n",
    "\n",
    "Therefore,\n",
    "$$\n",
    "J_k(x_k) = 2 \\sin\\!\\left(\\frac{2\\pi - x_k}{2 (N-k+1)}\\right)\n",
    "+ 2 (N-k) \\sin\\!\\left(\\frac{2\\pi - x_k}{2 (N-k+1)}\\right)\n",
    "$$\n",
    "\n",
    "Simplifying,\n",
    "$$\n",
    "J_k(x_k) = 2 (N-k+1) \\sin\\!\\left(\\frac{2\\pi - x_k}{2 (N-k+1)}\\right).\n",
    "$$\n",
    "\n",
    "**Conclusion.**\n",
    "By induction, for all $k=0,1,\\dots,N-1$, the optimal policy and cost-to-go are\n",
    "$$\n",
    "u_k^\\star = \\frac{2\\pi - x_k}{N-k+1},\n",
    "\\qquad\n",
    "J_k(x_k) = 2 (N-k+1) \\sin\\!\\left(\\frac{2\\pi - x_k}{2 (N-k+1)}\\right).\n",
    "$$\n",
    "\n",
    "---\n"
   ],
   "id": "fc162a852007ed79"
  },
  {
   "cell_type": "markdown",
   "id": "0ad8ec40",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "**e**. Starting from $x_1 = 0$, what is the optimal sequence of controls?\n",
    "\n",
    "Hopefully now you see why my original claim is true!\n",
    "\n",
    "**(TODO) ANSWER:**"
   ]
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "---\n",
    "**Answer: 1.e**\n",
    "\n",
    "The optimal control sequence is to make each split of equal size.\n",
    "\n",
    "At k=1:\n",
    "$$\n",
    "u_1^\\star = \\frac{2\\pi - x_1}{N} = \\frac{2\\pi}{N}.\n",
    "$$\n",
    "So the first control is $2\\pi/N$\n",
    "Then $x_2 = x_1 + u_1^\\star = 2\\pi/N$.\n",
    "Plugging into the formula:\n",
    "$$\n",
    "u_2^\\star = \\frac{2\\pi - x_2}{N-1}\n",
    "= \\frac{2\\pi - 2\\pi/N}{N-1}\n",
    "= \\frac{2\\pi}{N}.\n",
    "$$\n",
    "Same again.\n",
    "Repeating this, we find that\n",
    "$$\n",
    "u_k^\\star = \\frac{2\\pi}{N} \\quad \\text{for all } k=1,\\dots,N.\n",
    "$$\n",
    "\n",
    "---"
   ],
   "id": "6dcbc4ffda5ee71e"
  },
  {
   "cell_type": "markdown",
   "id": "33dfd1d1",
   "metadata": {},
   "source": [
    "### Bonus:\n",
    "We are not yet done for this exercise. Since you have probably already spent quite some time on this exercise, I will leave the rest of the exercise a bonus. In case you found this simple geometric problem interesting, you should keep reading as we will use numerical techniques to prove the same claim.\n",
    "\n",
    "In Fig. 2, by denoting\n",
    "\n",
    "$$\n",
    "u_N = 2\\pi - x_N = 2\\pi - (u_1 + \\cdots + u_{N-1})\n",
    "$$\n",
    "\n",
    "as the angle between the line $O — x_N$ and the line $O — x_1$, it is not hard to observe that the perimeter of the $N$-polygon is\n",
    "\n",
    "$$\n",
    "\\sum_{k=1}^N 2 \\sin\\!\\left(\\tfrac{u_k}{2}\\right).\n",
    "$$\n",
    "\n",
    "Consequently, to maximize the perimeter, we can formulate the following optimization\n",
    "\n",
    "$$\n",
    "\\max_{u_1,\\ldots,u_N} \\;\\; \\sum_{k=1}^N 2 \\sin\\!\\left(\\tfrac{u_k}{2}\\right)\n",
    "$$\n",
    "\n",
    "subject to\n",
    "\n",
    "$$\n",
    "u_k > 0, \\; k = 1, \\ldots, N, \\\\\n",
    "u_1 + \\cdots + u_N = 2\\pi\n",
    "\\tag{9.2}\n",
    "$$\n",
    "\n",
    "where $u_k$ can be seen as the angle spanned by the line $x_k — x_{k+1}$ with respect to the center $O$ so that they are positive and sum up to $2\\pi$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b21f4bb8",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "**f**. Show that the optimization (9.2) is convex.\n",
    "(Hint: first show the feasible set is convex, and then show the objective function is concave over the feasible set.)\n",
    "\n",
    "**(TODO) ANSWER:**\n",
    "\n",
    "Now that we have shown (9.2) is a convex optimization problem, we know that pretty much any numerical algorithm will guarantee convergence to the globally optimal solution.\n",
    "\n",
    "There are many numerical algorithms that can compute optimal solutions of an optimization problem (Nocedal and Wright 1999). Python provides a nice interface, `scipy.optimize`, to many such algorithms, and let us use `scipy.optimize` to solve (9.2) so we can numerically prove our claim."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5d86a56",
   "metadata": {},
   "source": [
    "\n",
    "**g**. We have provided most of the code necessary for solving (9.2) below. Please fill in the definition of the function `perimeter(u)`, and then run the code. Show your results for $N = 3, 10, 100$. Do the solutions obtained from Python verify our claim?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9ce6c03",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy.optimize import minimize\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# -------- Parameters --------\n",
    "N = 10  # Number of points\n",
    "\n",
    "# -------- Objective: polygon perimeter on the unit circle (edge length = 2*sin(u/2)) --------\n",
    "def perimeter(u):\n",
    "    ##############################\n",
    "    # TODO BLOCK\n",
    "    ##############################\n",
    "\n",
    "def neg_perimeter(u):\n",
    "# SciPy minimizes; negate to perform maximization\n",
    "    return -perimeter(u)\n",
    "\n",
    "# -------- Constraints & initialization --------\n",
    "# Linear equality: sum(a) = 2π\n",
    "eq_cons = {'type': 'eq', 'fun': lambda u: np.sum(u) - 2.0 * np.pi}\n",
    "\n",
    "# Bounds: u_i ∈ [0, 2π] (upper bound helps numerics)\n",
    "bounds = [(0.0, 2.0 * np.pi)] * N\n",
    "\n",
    "# Initial guess: positive random vector normalized to 2π\n",
    "rng = np.random.default_rng(0)\n",
    "u0 = rng.random(N)\n",
    "u0 = u0 / u0.sum() * 2.0 * np.pi\n",
    "\n",
    "# -------- Solve (SLSQP) --------\n",
    "res = minimize(\n",
    "    neg_perimeter, u0,\n",
    "    method='SLSQP',\n",
    "    bounds=bounds,\n",
    "    constraints=[eq_cons],\n",
    "    options={'maxiter': 2000, 'ftol': 1e-12, 'disp': True}\n",
    ")\n",
    "\n",
    "uopt = res.x\n",
    "print(\"Success:\", res.success, \"| message:\", res.message)\n",
    "print(\"Perimeter =\", perimeter(uopt))\n",
    "\n",
    "# -------- Recover vertex angles x by cumulative sum (x[0]=0; others accumulate preceding gaps) --------\n",
    "x = np.zeros(N)\n",
    "x[1:] = np.cumsum(uopt[:-1])\n",
    "\n",
    "# -------- Plot --------\n",
    "fig, ax = plt.subplots()\n",
    "# Draw unit circle\n",
    "circle = plt.Circle((0, 0), 1.0, fill=False)\n",
    "ax.add_patch(circle)\n",
    "\n",
    "# Scatter vertices\n",
    "ax.scatter(np.cos(x), np.sin(x), s=40, label=\"points\")\n",
    "\n",
    "ax.set_aspect('equal', adjustable='box')\n",
    "ax.set_xlim(-1.1, 1.1)\n",
    "ax.set_ylim(-1.1, 1.1)\n",
    "ax.set_xlabel(\"x\")\n",
    "ax.set_ylabel(\"y\")\n",
    "ax.set_title(f\"N={N}, perimeter={perimeter(uopt):.6f}\")\n",
    "ax.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a181bad5",
   "metadata": {},
   "source": [
    "## 2. Convergence proof of Value iteration\n",
    "\n",
    "Let the Bellman optimality operator be\n",
    "$$\n",
    "(T^\\star V)(s)=\\max_a\\Big[\\,R(s,a)+\\gamma\\sum_{s'}P(s'|s,a)V(s')\\,\\Big],\n",
    "\\qquad \\gamma\\in[0,1).\n",
    "$$\n",
    "Let $V^\\star$ denote the optimal value function, i.e., $V^\\star=T^\\star V^\\star$.\n",
    "Value iteration is $V_{k+1}=T^\\star V_k$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "553b1668",
   "metadata": {},
   "source": [
    "### 2.1 Contraction\n",
    "\n",
    "We first prove the operator is a $\\gamma$-**contraction**, i.e.\n",
    "$$\n",
    "||V_{k+1}-V^\\star|| \\leq \\gamma ||V_k-V^\\star||\n",
    "$$\n",
    "\n",
    "**(TODO) Answer:**\n"
   ]
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "---\n",
    "Answer: 2.1\n",
    "The idea is to show that the Bellman optimality operator $T^\\star$ brings value functions closer together by at least a factor of $\\gamma$.\n",
    "\n",
    "Value iteration is defined by\n",
    "\n",
    "$$\n",
    "V_{k+1} = T^\\star V_k,\n",
    "\\qquad V^\\star = T^\\star V^\\star.\n",
    "$$\n",
    "\n",
    "So the difference at step \\(k+1\\) is\n",
    "\n",
    "$$\n",
    "\\|V_{k+1} - V^\\star\\| = \\|T^\\star V_k - T^\\star V^\\star\\|.\n",
    "$$\n",
    "\n",
    "For a given \\(s\\),\n",
    "\n",
    "$$\n",
    "(T^\\star V)(s) = \\max_a \\Big[ R(s,a) + \\gamma \\sum_{s'} P(s'|s,a)\\, V(s') \\Big].\n",
    "$$\n",
    "\n",
    "Now compare \\(T^\\star V_k\\) and \\(T^\\star V^\\star\\):\n",
    "\n",
    "$$\n",
    "|(T^\\star V_k)(s) - (T^\\star V^\\star)(s)|\n",
    "= \\Big|\\max_a Q_k(s,a) - \\max_a Q^\\star(s,a)\\Big|,\n",
    "$$\n",
    "\n",
    "where \\(Q_k(s,a)\\) and \\(Q^\\star(s,a)\\) are the bracketed terms using \\(V_k\\) and \\(V^\\star\\).\n",
    "\n",
    "We know from the triangle inequality theorem that:\n",
    "\n",
    "$$\n",
    "\\big|\\max_a x_a - \\max_a y_a\\big| \\le \\max_a |x_a - y_a|.\n",
    "$$\n",
    "\n",
    "So we can bound the difference by\n",
    "\n",
    "$$\n",
    "|(T^\\star V_k)(s) - (T^\\star V^\\star)(s)|\n",
    "\\le \\max_a \\left| \\gamma \\sum_{s'} P(s'|s,a)\\big(V_k(s') - V^\\star(s')\\big) \\right|.\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\le \\gamma \\max_a \\sum_{s'} P(s'|s,a)\\, |V_k(s') - V^\\star(s')|.\n",
    "$$\n",
    "\n",
    "Since\n",
    "\n",
    "$$\n",
    "|V_k(s') - V^\\star(s')| \\le \\|V_k - V^\\star\\|\n",
    "$$\n",
    "\n",
    "for every \\(s'\\), and the probabilities sum to \\(1\\), we get\n",
    "\n",
    "$$\n",
    "|(T^\\star V_k)(s) - (T^\\star V^\\star)(s)| \\le \\gamma \\|V_k - V^\\star\\|.\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\|T^\\star V_k - T^\\star V^\\star\\| \\le \\gamma \\|V_k - V^\\star\\|.\n",
    "$$\n",
    "\n",
    "This is exactly the contraction property.\n"
   ],
   "id": "3e91712f1db546d6"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### 2.2 linear convergence\n",
    "Next we prove the convergence is actually **linear**, i.e.\n",
    "$$\n",
    "\\|V_k-V^\\star\\|_\\infty \\leq \\gamma^k \\|V_0-V^\\star\\|_\\infty\n",
    "$$\n",
    "\n",
    "**(TODO) Answer:**\n"
   ],
   "id": "90b835d5"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "---\n",
    "**Answer: 2.2**\n",
    "\n",
    "We show that value iteration converges **linearly** with rate \\(\\gamma\\):\n",
    "$$\n",
    "\\|V_k - V^\\star\\|_\\infty \\le \\gamma^k \\|V_0 - V^\\star\\|_\\infty.\n",
    "$$\n",
    "\n",
    "**Proof (via contraction, by induction).** From §2.1 we have the contraction:\n",
    "\\[\n",
    "\\|V_{k+1} - V^\\star\\|_\\infty \\le \\gamma \\,\\|V_k - V^\\star\\|_\\infty, \\qquad 0\\le\\gamma<1.\n",
    "\\]\n",
    "\n",
    "- **Base case (\\(k=0\\)).** Trivial: $\\|V_0 - V^\\star\\|_\\infty \\le \\gamma^0 \\|V_0 - V^\\star\\|_\\infty$\n",
    "\n",
    "- **Inductive step.** Assume $\\|V_k - V^\\star\\|_\\infty \\le \\gamma^k \\|V_0 - V^\\star\\|_\\infty$. Then\n",
    "$$\n",
    "\\|V_{k+1} - V^\\star\\|_\\infty\n",
    "\\;\\le\\; \\gamma \\|V_k - V^\\star\\|_\\infty\n",
    "\\;\\le\\; \\gamma \\cdot \\gamma^k \\|V_0 - V^\\star\\|_\\infty\n",
    "\\;=\\; \\gamma^{k+1} \\|V_0 - V^\\star\\|_\\infty.\n",
    "$$\n",
    "Thus the claim holds for all $k\\ge 0$\n",
    "\n",
    "---"
   ],
   "id": "318a6bcf4a23ecc8"
  },
  {
   "cell_type": "markdown",
   "id": "3bd797a5",
   "metadata": {},
   "source": [
    "### 2.3 Practical stopping rule\n",
    "\n",
    "In practice we never know what is the true $V^\\star$. But what we can calculate is the difference between two iterations. Here we (1) prove an error bound of $\\|V-V^\\star\\|_\\infty$ by $\\|V_{k+1} - V_k\\|_\\infty$:\n",
    "\n",
    "$$\n",
    "\\|V_k-V^\\star\\|_\\infty \\leq \\frac{\\|V_{k+1} - V_k\\|_\\infty}{1-\\gamma}\n",
    "$$\n",
    "\n",
    "and (2) Compute the tolerance on the consecutive-iterate gap $\\|V_{k+1}-V_k\\|_\\infty$ needed to guarantee $\\|V - V^\\star\\|_\\infty \\le 10^{-6}$ when $\\gamma=0.99$.\n",
    "\n",
    "**(TODO) Answer:**\n"
   ]
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "---\n",
    "**Answer: 2.3 (a)**\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "|V - V^\\star\\|_\\infty\n",
    "&= \\|V - T^\\star V^\\star\\|_\\infty \\\\\n",
    "&\\le \\|V - T^\\star V\\|_\\infty \\;+\\; \\|T^\\star V - T^\\star V^\\star\\|_\\infty \\\\\n",
    "&\\le \\|T^\\star V - V\\|_\\infty \\;+\\; \\gamma \\|V - V^\\star\\|_\\infty.\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "Rearrange:\n",
    "$(1-\\gamma)\\,\\|V - V^\\star\\|_\\infty \\;\\le\\; \\|T^\\star V - V\\|_\\infty$\n",
    "which proves the error bound.\n",
    "\n",
    "Now plug in $V=V_k$. Since $V_{k+1}=T^\\star V_k$,\n",
    "$$\n",
    "\\|V_k - V^\\star\\|_\\infty\n",
    "\\;\\le\\; \\frac{\\|T^\\star V_k - V_k\\|_\\infty}{1-\\gamma}\n",
    "\\;=\\; \\frac{\\|V_{k+1}-V_k\\|_\\infty}{1-\\gamma}.\n",
    "$$\n",
    "\n",
    "**Answer: 2.3(b)**\n",
    "We want $\\|V_k - V^\\star\\|_\\infty \\le 10^{-6}$ when $\\gamma=0.99$.\n",
    "Using the bound above, it suffices to ensure\n",
    "$$\n",
    "\\frac{\\|V_{k+1}-V_k\\|_\\infty}{1-0.99} \\;\\le\\; 10^{-6}\n",
    "\\;\\;\\Longleftrightarrow\\;\\;\n",
    "\\|V_{k+1}-V_k\\|_\\infty \\;\\le\\; (1-0.99)\\,10^{-6} \\;=\\; 10^{-8}.\n",
    "$$\n",
    "\n",
    "---"
   ],
   "id": "ed3863a300c17bf0"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### 2.4 The bound of iterations\n",
    "\n",
    "Assume $\\|V_1 - V_0\\|_\\infty = 1$, $\\gamma = 0.99$. How much iterations do we need to have $\\|V_k - V^\\star\\|_\\infty \\leq 10^{-6}$?\n",
    "\n",
    "**(TODO) Answer:**\n",
    "\n"
   ],
   "id": "3a065e0a"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "---\n",
    "**Answer: 2.4**\n",
    "\n",
    "We know from the **linear convergence result** (§2.2):\n",
    "\n",
    "$$\n",
    "|V_k - V^\\star\\|_\\infty \\;\\le\\; \\gamma^k \\,\\|V_0 - V^\\star\\|_\\infty.\n",
    "$$\n",
    "\n",
    "But we don’t know $\\|V_0 - V^\\star\\|_\\infty$. Instead, we use the **error bound** (§2.3):\n",
    "\n",
    "$$\n",
    "\\|V_0 - V^\\star\\|_\\infty \\;\\le\\; \\frac{\\|V_1 - V_0\\|_\\infty}{1-\\gamma}.\n",
    "$$\n",
    "\n",
    "By assumption, $\\|V_1 - V_0\\|_\\infty = 1$ and $\\gamma=0.99$\n",
    "So\n",
    "$$\n",
    "\\|V_0 - V^\\star\\|_\\infty \\;\\le\\; \\frac{1}{1-0.99} \\;=\\; 100.\n",
    "$$\n",
    "\n",
    "For all \\(k\\),\n",
    "$$\n",
    "\\|V_k - V^\\star\\|_\\infty \\;\\le\\; \\gamma^k \\cdot 100.\n",
    "$$\n",
    "\n",
    "We want\n",
    "$$\n",
    "100 \\cdot \\gamma^k \\;\\le\\; 10^{-6}.\n",
    "$$\n",
    "\n",
    "Take logs (with $\\gamma=0.99$):\n",
    "\n",
    "$$\n",
    "\\gamma^k \\;\\le\\; 10^{-8}\n",
    "\\quad\\Longrightarrow\\quad\n",
    "k \\;\\ge\\; \\frac{\\ln(10^{-8})}{\\ln(0.99)}.\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\ln(10^{-8}) = -18.4207,\n",
    "\\qquad \\ln(0.99) \\approx -0.0100503\n",
    "$$\n",
    "\n",
    "So\n",
    "$$\n",
    "k \\;\\ge\\; \\frac{-18.4207}{-0.0100503} \\;\\approx\\; 1833.\n",
    "$$\n",
    "\n",
    "With $\\|V_1 - V_0\\|_\\infty=1$ and $\\gamma=0.99$, we need about\n",
    "\n",
    "$$\n",
    "k \\;\\approx\\; 1833\n",
    "$$\n",
    "\n",
    "iterations to guarantee $\\|V_k - V^\\star\\|_\\infty \\le 10^{-6}$\n",
    "\n",
    "---"
   ],
   "id": "c315866f77ba4c66"
  },
  {
   "cell_type": "markdown",
   "id": "dd169530",
   "metadata": {},
   "source": [
    "## 3. Cliffwalk\n",
    "\n",
    "Implement policy evaluation, policy improvement, value iteration, and policy iteration for the `CliffWalking` task. For clarity and reproducibility, We include a minimal reimplementation of the environment that mirrors Gymnasium’s dynamics and reward scheme.\n",
    "\n",
    "\n",
    "<figure style=\"text-align:center;\">\n",
    "  <img src=\"https://raw.githubusercontent.com/ComputationalRobotics/2025-ES-AM-158-PSET/main/PSET1/cliffwalk.png\" width=\"600\" alt=\"Inscribed polygon\">\n",
    "  <figcaption style=\"color:#6a737d; font-style:italic;\">\n",
    "    Figure 3. Illustration to cliffwalk problem.\n",
    "  </figcaption>\n",
    "</figure>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5782a72c",
   "metadata": {},
   "source": [
    "**CliffWalking (Gym-compatible) — Specification**\n",
    "\n",
    "- **Grid:** 4 rows × 12 columns (row-major indexing; `state_id = row * 12 + col`; index origin at top-left in comments).\n",
    "- **Start:** bottom-left cell `(row=3, col=0)`.\n",
    "- **Goal:** bottom-right cell `(row=3, col=11)`.\n",
    "- **Actions (4):** up (0), right (1), down (2), left (3).\n",
    "- **Rewards:** −1 per step; −100 on entering a cliff cell; 0 at the goal.\n",
    "- **Termination:** episode ends upon reaching the goal; this states are terminal/absorbing. If reaching cliff will go back to start.\n",
    "\n",
    "**Transition table**\n",
    "\n",
    "- `P[state][action] → list[(prob, next_state, reward, done)]`\n",
    "- Deterministic dynamics: each list contains a single tuple with `prob = 1.0` after handling boundaries, cliff, and goal."
   ]
  },
  {
   "cell_type": "code",
   "id": "3422e418",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-26T22:11:23.786950Z",
     "start_time": "2025-09-26T22:11:23.782124Z"
    }
   },
   "source": [
    "import copy\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import gymnasium as gym\n",
    "\n",
    "np.set_printoptions(precision=3, suppress=True)\n",
    "\n",
    "# Create Gym CliffWalking environment (v1).\n",
    "env_gym = gym.make(\"CliffWalking-v1\", render_mode=\"ansi\")\n",
    "nS, nA = env_gym.observation_space.n, env_gym.action_space.n\n",
    "# The CliffWalking grid is 4 × 12; actions are 0=UP, 1=RIGHT, 2=DOWN, 3=LEFT.\n",
    "print(f\"State count={nS}, Action count={nA}\")\n",
    "\n",
    "# -------------------------------------------------------------------\n",
    "# Utility: pretty-print a value function as a 2D grid (nrow × ncol).\n",
    "# Values can be any (nS,) array-like; states are indexed row-major:\n",
    "# State_id = row * ncol + col\n",
    "# -------------------------------------------------------------------\n",
    "def print_values(values, nrow: int, ncol: int, title: str = \"State Values\"):\n",
    "    \"\"\"Print a value table in grid form.\"\"\"\n",
    "    values = np.asarray(values).reshape(nrow, ncol)\n",
    "    print(title)\n",
    "    for r in range(nrow):\n",
    "        print(\" \".join(f\"{values[r, c]:6.2f}\" for c in range(ncol)))\n",
    "    print()\n",
    "\n",
    "# -------------------------------------------------------------------\n",
    "# Utility: pretty-print a policy on the CliffWalking grid.\n",
    "# \n",
    "# Accepted pi formats for each state s:\n",
    "# - Int a               : deterministic action\n",
    "# - Length-4 vector     : Q-values or preferences; we render argmax (ties shown)\n",
    "# - Length-4 probabilities (stochastic policy): greedy action(s) by max prob\n",
    "# \n",
    "# Notes:\n",
    "# - Uses Gym's action order: 0=UP(↑), 1=RIGHT(→), 2=DOWN(↓), 3=LEFT(←)\n",
    "# - Terminal states in CliffWalking (bottom row except col=0) are marked:\n",
    "# S at (last_row, 0), C for cliff cells (last_row, 1..ncol-2), G at (last_row, ncol-1)\n",
    "# -------------------------------------------------------------------\n",
    "def print_policy(pi, nrow: int, ncol: int, title: str = \"Policy\"):\n",
    "    \"\"\"Print a deterministic/stochastic policy.\n",
    "    - If pi is a list of lists (length 4): treat as stochastic over [up, down, left, right].\n",
    "    - We render the greedy direction; if ties exist, we list all best arrows.\n",
    "    \"\"\"\n",
    "    arrow = {0:\"^\", 1:\">\", 2:\"v\", 3:\"<\"}  # Order aligned with env actions in this notebook\n",
    "    print(title)\n",
    "    for i in range(nrow):\n",
    "        row_syms = []\n",
    "        for j in range(ncol):\n",
    "            s = i*ncol + j\n",
    "            p = pi[s]\n",
    "            # Determine best action(s)\n",
    "            if isinstance(p, list) and len(p) == 4:\n",
    "                best = np.argwhere(np.array(p) == np.max(p)).flatten().tolist()\n",
    "            elif isinstance(p, int):\n",
    "                best = [p]\n",
    "            else:\n",
    "                # Fallback: greedy over provided vector/array\n",
    "                arr = np.array(p, dtype=float).ravel()\n",
    "                best = np.argwhere(arr == np.max(arr)).flatten().tolist()\n",
    "            # Special case: terminals on bottom row except j==0\n",
    "            if i == nrow-1 and j > 0:\n",
    "                row_syms.append(\"T\")\n",
    "            else:\n",
    "                row_syms.append(\"\".join(arrow[a] for a in best))\n",
    "        print(\" \".join(sym if sym else \".\" for sym in row_syms))\n",
    "    print(p)\n",
    "    print()"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "State count=48, Action count=4\n"
     ]
    }
   ],
   "execution_count": 6
  },
  {
   "cell_type": "markdown",
   "id": "cc0f7f98",
   "metadata": {},
   "source": [
    "### 3.1 Define Environment Model (no need to fill in)"
   ]
  },
  {
   "cell_type": "code",
   "id": "48012a41",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-26T22:11:23.801272Z",
     "start_time": "2025-09-26T22:11:23.797740Z"
    }
   },
   "source": [
    "class CliffWalkingEnv:\n",
    "    \"\"\"Cliff Walking environment (Gym-compatible dynamics)\n",
    "\n",
    "    State indexing\n",
    "    --------------\n",
    "    - Flattened row-major: state_id = row * ncol + col\n",
    "    - Rows: 0..nrow-1 (top → bottom), Cols: 0..ncol-1 (left → right)\n",
    "\n",
    "    Actions (match Gym/toy_text)\n",
    "    ----------------------------\n",
    "    - 0: UP (↑), 1: RIGHT (→), 2: DOWN (↓), 3: LEFT (←)\n",
    "\n",
    "    Grid (nrow=4, ncol=12)\n",
    "    ----------------------\n",
    "        [  0] [  1] [  2] [  3] [  4] [  5] [  6] [  7] [  8] [  9] [ 10] [ 11]\n",
    "        [ 12] [ 13] [ 14] [ 15] [ 16] [ 17] [ 18] [ 19] [ 20] [ 21] [ 22] [ 23]\n",
    "        [ 24] [ 25] [ 26] [ 27] [ 28] [ 29] [ 30] [ 31] [ 32] [ 33] [ 34] [ 35]\n",
    "        [36=S] [37=C] [38=C] [39=C] [40=C] [41=C] [42=C] [43=C] [44=C] [45=C] [46=C] [47=G]\n",
    "\n",
    "    Legend\n",
    "    ------\n",
    "    - S (start):  (row=3, col=0)   -> state 36\n",
    "    - C (cliff):  (row=3, col=1..10) -> states 37..46\n",
    "    - G (goal):   (row=3, col=11)  -> state 47\n",
    "\n",
    "    Termination & rewards\n",
    "    ---------------------\n",
    "    - Stepping into a cliff cell: reward = -100, done = False, go back to start\n",
    "    - Any other move:             reward = -1,   done = False\n",
    "    - Terminal states are absorbing: once in {goal}, any action keeps you there with reward 0.\n",
    "    \"\"\"\n",
    "\n",
    "    # Action constants for clarity\n",
    "    A_UP, A_RIGHT, A_DOWN, A_LEFT = 0, 1, 2, 3\n",
    "\n",
    "    def __init__(self, ncol: int = 12, nrow: int = 4):\n",
    "        self.ncol = int(ncol)\n",
    "        self.nrow = int(nrow)\n",
    "        self.nS = self.nrow * self.ncol\n",
    "        self.nA = 4\n",
    "        # Transition table: P[state][action] = [(prob, next_state, reward, done)]\n",
    "        self.P = self._create_P()\n",
    "\n",
    "    def _create_P(self):\n",
    "    # Allocate empty transition table\n",
    "        P = [[[] for _ in range(self.nA)] for _ in range(self.nS)]\n",
    "\n",
    "        # Movement deltas in (dx, dy), matching action order: 0↑, 1→, 2↓, 3←\n",
    "        # NOTE: x increases to the right (columns), y increases downward (rows).\n",
    "        deltas = {\n",
    "            self.A_UP:    ( 0, -1),\n",
    "            self.A_RIGHT: ( 1,  0),  # (1, 0) Written to hint order; same as (1, 0)\n",
    "            self.A_DOWN:  ( 0,  1),\n",
    "            self.A_LEFT:  (-1,  0),\n",
    "        }\n",
    "\n",
    "        start_s = (self.nrow - 1) * self.ncol + 0\n",
    "        goal_s  = (self.nrow - 1) * self.ncol + (self.ncol - 1)\n",
    "\n",
    "        for r in range(self.nrow):\n",
    "            for c in range(self.ncol):\n",
    "                s = r * self.ncol + c\n",
    "\n",
    "                if r == self.nrow - 1 and c > 0:\n",
    "                    for a in range(self.nA):\n",
    "                        P[s][a] = [(1.0, s, 0.0, True)]\n",
    "                    continue\n",
    "\n",
    "                for a in range(self.nA):\n",
    "                    dx, dy = deltas[a]\n",
    "\n",
    "                    nc = min(self.ncol - 1, max(0, c + dx))\n",
    "                    nr = min(self.nrow - 1, max(0, r + dy))\n",
    "\n",
    "                    ns = nr * self.ncol + nc\n",
    "                    reward = -1.0\n",
    "                    done = False\n",
    "\n",
    "                    if nr == self.nrow - 1 and 1 <= nc <= self.ncol - 2:\n",
    "                        ns = start_s          \n",
    "                        reward = -100.0\n",
    "                        done = False\n",
    "\n",
    "                    elif nr == self.nrow - 1 and nc == self.ncol - 1:\n",
    "                        done = True\n",
    "\n",
    "                    P[s][a] = [(1.0, ns, reward, done)]\n",
    "\n",
    "        return P\n",
    "\n",
    "\n",
    "# Build environment\n",
    "env = CliffWalkingEnv(ncol=12, nrow=4)"
   ],
   "outputs": [],
   "execution_count": 7
  },
  {
   "cell_type": "markdown",
   "id": "572fedfe",
   "metadata": {},
   "source": [
    "### 3.2 Policy Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "id": "72beca54",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-26T22:22:53.547920Z",
     "start_time": "2025-09-26T22:22:53.531339Z"
    }
   },
   "source": [
    "def policy_evaluation(env, pi, gamma=0.95, theta=1e-10):\n",
    "    \"\"\"Iterative policy evaluation for a given stochastic policy π(a|s).\n",
    "\n",
    "    Args:\n",
    "        env: environment with a tabular transition model env.P where\n",
    "             P[s][a] = [(prob, next_state, reward, done)].\n",
    "        pi:  policy probabilities shaped [nS][4]; each pi[s] is a length-4 list\n",
    "             over actions [UP, RIGHT, DOWN, LEFT].\n",
    "        gamma: discount factor ∈ [0, 1).\n",
    "        theta: convergence threshold on the ∞-norm of value updates.\n",
    "\n",
    "    Returns:\n",
    "        v: list of state values of length nS.\n",
    "    \"\"\"\n",
    "    nS = env.nrow * env.ncol\n",
    "    v = [0.0] * nS  # Initialize V(s)=0\n",
    "    it = 1  # Iteration counter (logging only)\n",
    "\n",
    "    while True:\n",
    "        max_diff = 0.0\n",
    "        new_v = [0.0] * nS\n",
    "\n",
    "        for s in range(nS):\n",
    "            v_sum = 0.0  # Σ_a π(a|s) * Q(s,a)\n",
    "            for a in range(4):\n",
    "                pi_sa = pi[s][a]\n",
    "                for (prob, next_state, reward, done) in env.P[s][a]:\n",
    "                    v_sum += pi_sa * prob * (reward + (0.0 if done else gamma * v[next_state]))\n",
    "\n",
    "            new_v[s] = v_sum\n",
    "            max_diff = max(max_diff, abs(new_v[s] - v[s]))\n",
    "\n",
    "        v = new_v\n",
    "        if max_diff < theta:\n",
    "            break\n",
    "        it += 1\n",
    "\n",
    "    print(f\"Policy evaluation converged in {it} iteration(s).\")\n",
    "    return v\n",
    "\n",
    "\n",
    "# --- Example: evaluate a uniform random policy ---\n",
    "pi = [[0.25, 0.25, 0.25, 0.25] for _ in range(env.nrow * env.ncol)]\n",
    "gamma = 0.95\n",
    "\n",
    "v = policy_evaluation(env, pi, gamma)\n",
    "\n",
    "# Pretty-print the value function as a 4×12 grid\n",
    "print_values(v, env.nrow, env.ncol, title=\"Value Function under Random Policy\")"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Policy evaluation converged in 494 iteration(s).\n",
      "Value Function under Random Policy\n",
      "-143.21 -147.36 -151.35 -153.93 -155.11 -155.06 -153.67 -150.46 -144.47 -134.46 -119.99 -105.22\n",
      "-164.99 -174.34 -180.41 -183.52 -184.80 -184.83 -183.63 -180.70 -174.70 -163.02 -141.34 -108.39\n",
      "-207.96 -237.09 -246.20 -249.36 -250.43 -250.52 -249.79 -247.82 -243.20 -231.68 -199.50 -97.21\n",
      "-261.35   0.00   0.00   0.00   0.00   0.00   0.00   0.00   0.00   0.00   0.00   0.00\n",
      "\n"
     ]
    }
   ],
   "execution_count": 16
  },
  {
   "cell_type": "markdown",
   "id": "da66ff59",
   "metadata": {},
   "source": [
    "### 3.3 Policy Iteration\n",
    "\n",
    "Policy Iteration alternates between:\n",
    "1) **Policy Evaluation**: compute the state-value function $V^{\\pi}$ of the current policy $\\pi$\n",
    "2) **Policy Improvement**: update $\\pi$ to be greedy w.r.t. $V^{\\pi}$"
   ]
  },
  {
   "cell_type": "code",
   "id": "37e91266",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-26T22:30:47.254091Z",
     "start_time": "2025-09-26T22:30:47.139288Z"
    }
   },
   "source": [
    "def policy_improvement(env, pi, v, gamma=0.95):\n",
    "    \"\"\"Greedy policy improvement w.r.t. the current state-value function V.\n",
    "\n",
    "    For each state s:\n",
    "      1) Compute Q(s,a) = Σ_{s'} P(s'|s,a)[ r(s,a,s') + γ V(s') ] for all a.\n",
    "      2) Find the action(s) with maximal Q(s,a).\n",
    "      3) Update π(·|s) to split probability uniformly among all maximizers (tie-aware).\n",
    "\n",
    "    Args:\n",
    "        env: Tabular environment with transitions env.P where\n",
    "             P[s][a] = [(prob, next_state, reward, done)].\n",
    "        pi:  Current (possibly stochastic) policy, shape [nS][4]; updated in-place.\n",
    "        v:   Current state-value function V(s), length nS.\n",
    "        gamma: Discount factor.\n",
    "\n",
    "    Returns:\n",
    "        pi: The improved policy (same object, updated in-place).\n",
    "    \"\"\"\n",
    "    nS = env.nrow * env.ncol\n",
    "    nA = 4\n",
    "    eps = 1e-8  # Numerical tolerance for tie-breaking\n",
    "\n",
    "    for s in range(nS):\n",
    "        q_list = []\n",
    "        for a in range(nA):\n",
    "            q_sa = 0.0\n",
    "            for (prob, next_state, reward, done) in env.P[s][a]:\n",
    "                q_sa += prob * (reward + (0.0 if done else gamma * v[next_state]))\n",
    "            q_list.append(q_sa)\n",
    "        best_a = np.argmax(q_list)\n",
    "\n",
    "        pi[s] = [1.0 if a == best_a else 0.0 for a in range(nA)]\n",
    "\n",
    "    print(\"Policy improvement completed.\")\n",
    "    return pi\n",
    "\n",
    "\n",
    "# --- Policy Iteration loop ---\n",
    "pi = [[0.25, 0.25, 0.25, 0.25] for _ in range(env.nrow * env.ncol)]\n",
    "iters = 0\n",
    "while True:\n",
    "    v = policy_evaluation(env, pi, gamma)\n",
    "    old_pi = copy.deepcopy(pi)\n",
    "    new_pi = policy_improvement(env, pi, v, gamma)\n",
    "    iters += 1\n",
    "    if old_pi == new_pi:  # Policy is stable\n",
    "        print(f\"Policy iteration converged in {iters} improvement step(s).\")\n",
    "        break\n",
    "\n",
    "# Report results\n",
    "print_values(v, env.nrow, env.ncol, title=\"Optimal Value Function\")\n",
    "print_policy(pi, env.nrow, env.ncol, title=\"Optimal Policy\")"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Policy evaluation converged in 494 iteration(s).\n",
      "Policy improvement completed.\n",
      "Policy evaluation converged in 450 iteration(s).\n",
      "Policy improvement completed.\n",
      "Policy evaluation converged in 450 iteration(s).\n",
      "Policy improvement completed.\n",
      "Policy evaluation converged in 450 iteration(s).\n",
      "Policy improvement completed.\n",
      "Policy evaluation converged in 450 iteration(s).\n",
      "Policy improvement completed.\n",
      "Policy evaluation converged in 450 iteration(s).\n",
      "Policy improvement completed.\n",
      "Policy evaluation converged in 450 iteration(s).\n",
      "Policy improvement completed.\n",
      "Policy evaluation converged in 450 iteration(s).\n",
      "Policy improvement completed.\n",
      "Policy evaluation converged in 450 iteration(s).\n",
      "Policy improvement completed.\n",
      "Policy evaluation converged in 450 iteration(s).\n",
      "Policy improvement completed.\n",
      "Policy evaluation converged in 450 iteration(s).\n",
      "Policy improvement completed.\n",
      "Policy evaluation converged in 450 iteration(s).\n",
      "Policy improvement completed.\n",
      "Policy evaluation converged in 15 iteration(s).\n",
      "Policy improvement completed.\n",
      "Policy iteration converged in 13 improvement step(s).\n",
      "Optimal Value Function\n",
      "-10.25  -9.73  -9.19  -8.62  -8.03  -7.40  -6.73  -6.03  -5.30  -4.52  -3.71  -2.85\n",
      " -9.73  -9.19  -8.62  -8.03  -7.40  -6.73  -6.03  -5.30  -4.52  -3.71  -2.85  -1.95\n",
      " -9.19  -8.62  -8.03  -7.40  -6.73  -6.03  -5.30  -4.52  -3.71  -2.85  -1.95  -1.00\n",
      " -9.73   0.00   0.00   0.00   0.00   0.00   0.00   0.00   0.00   0.00   0.00   0.00\n",
      "\n",
      "Optimal Policy\n",
      "> > > > > > > > > > > v\n",
      "> > > > > > > > > > > v\n",
      "> > > > > > > > > > > v\n",
      "^ T T T T T T T T T T T\n",
      "[1.0, 0.0, 0.0, 0.0]\n",
      "\n"
     ]
    }
   ],
   "execution_count": 20
  },
  {
   "cell_type": "markdown",
   "id": "569b7f8d",
   "metadata": {},
   "source": [
    "### 3.4 Value Iteration\n",
    "\n",
    "Value Iteration applies **Bellman optimality** updates directly to $V$. Or one can treat value iteration as one step policy evaluation plus one step policy improvement.\n",
    "\n",
    "After convergence, extract the greedy policy."
   ]
  },
  {
   "cell_type": "code",
   "id": "4065588e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-26T22:53:27.009789Z",
     "start_time": "2025-09-26T22:53:26.999725Z"
    }
   },
   "source": [
    "def iterate(env, gamma=0.95, theta=1e-10):\n",
    "    \"\"\"Value Iteration.\n",
    "\n",
    "    Updates V(s) <- max_a Σ_{s'} P(s'|s,a) [ r(s,a,s') + γ V(s') ]\n",
    "    until the maximum state-wise change is below `theta`.\n",
    "\n",
    "    Args:\n",
    "        env: Tabular environment exposing env.P with\n",
    "             P[s][a] = [(prob, next_state, reward, done)] and grid sizes nrow, ncol.\n",
    "        gamma (float): Discount factor in [0, 1).\n",
    "        theta (float): Convergence threshold on the infinity-norm of value updates.\n",
    "\n",
    "    Returns:\n",
    "        list[float]: The converged state-value function V of length nS (= nrow * ncol).\n",
    "\n",
    "    Notes:\n",
    "        - Terminal states are modeled as absorbing with reward 0 in `env.P`.\n",
    "          The Bellman backup naturally yields V(terminal) = 0.\n",
    "        - `deltas` (max per-iteration change) is tracked for debugging but not returned.\n",
    "    \"\"\"\n",
    "    nS, nA = env.nrow * env.ncol, 4\n",
    "    deltas = []\n",
    "    iters = 0\n",
    "    v = [0.0] * nS\n",
    "\n",
    "    while True:\n",
    "        iters += 1\n",
    "        max_diff = 0.0\n",
    "        new_v = [0.0] * nS\n",
    "\n",
    "        for s in range(nS):\n",
    "            # Bellman optimality backup: V(s) = max_a Q(s,a)\n",
    "            q_list = []\n",
    "            for a in range(nA):\n",
    "                q_sa = 0.0\n",
    "                for (prob, next_state, reward, done) in env.P[s][a]:\n",
    "                    q_sa = prob * (reward + (0.0 if done else gamma * v[next_state]))\n",
    "                    q_list.append(q_sa)\n",
    "\n",
    "            new_v[s] = max(q_list)\n",
    "            max_diff = max(max_diff, abs(new_v[s] - v[s]))\n",
    "\n",
    "        v = new_v\n",
    "        deltas.append(max_diff)\n",
    "        if max_diff < theta:\n",
    "            break\n",
    "    print(iters)\n",
    "    return v\n",
    "\n",
    "\n",
    "def greedy_policy(env, v, gamma=0.95):\n",
    "    \"\"\"Extract a greedy (tie-aware) policy from a value function.\n",
    "\n",
    "    For each state s, compute Q(s,a) and set π(a|s)=1/k for all actions a that\n",
    "    achieve the maximal Q-value (ties split uniformly); 0 otherwise.\n",
    "\n",
    "    Args:\n",
    "        env: Tabular environment with env.P.\n",
    "        v (list[float]): State-value function V(s).\n",
    "        gamma (float): Discount factor.\n",
    "\n",
    "    Returns:\n",
    "        list[list[float]]: Policy π of shape [nS][4], each row summing to 1.\n",
    "    \"\"\"\n",
    "    nS, nA = env.nrow * env.ncol, 4\n",
    "    pi = [[0.0] * nA for _ in range(nS)]\n",
    "    eps = 1e-8  # Numerical tolerance for tie detection\n",
    "\n",
    "    for s in range(nS):\n",
    "        q_list = []\n",
    "        for a in range(nA):\n",
    "            q = 0.0\n",
    "            for (p, next_state, r, done) in env.P[s][a]:\n",
    "                q += p * (r if done else r + gamma * v[next_state])\n",
    "            q_list.append(q)\n",
    "\n",
    "        q_list = np.array(q_list, dtype=float)\n",
    "        max_q = q_list.max()\n",
    "        # Tie-aware argmax\n",
    "        opt_u = np.isclose(q_list, max_q, rtol=0.0, atol=eps)\n",
    "        k = int(opt_u.sum())\n",
    "        pi[s] = (opt_u / k).astype(float).tolist()\n",
    "\n",
    "    return pi\n",
    "\n",
    "\n",
    "# ----- Run Value Iteration and extract greedy policy -----\n",
    "gamma = 0.95  # Discount factor\n",
    "v = iterate(env, gamma=gamma)  # Assumes `env` is already constructed\n",
    "pi = greedy_policy(env, v, gamma=gamma)\n",
    "\n",
    "# Pretty-print results (assumes `print_values` and `print_policy` are defined)\n",
    "print_values(v, env.nrow, env.ncol, title=\"Optimal Value Function (Value Iteration)\")\n",
    "print_policy(pi, env.nrow, env.ncol, title=\"Optimal Policy (Value Iteration)\")"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15\n",
      "Optimal Value Function (Value Iteration)\n",
      "-10.25  -9.73  -9.19  -8.62  -8.03  -7.40  -6.73  -6.03  -5.30  -4.52  -3.71  -2.85\n",
      " -9.73  -9.19  -8.62  -8.03  -7.40  -6.73  -6.03  -5.30  -4.52  -3.71  -2.85  -1.95\n",
      " -9.19  -8.62  -8.03  -7.40  -6.73  -6.03  -5.30  -4.52  -3.71  -2.85  -1.95  -1.00\n",
      " -9.73   0.00   0.00   0.00   0.00   0.00   0.00   0.00   0.00   0.00   0.00   0.00\n",
      "\n",
      "Optimal Policy (Value Iteration)\n",
      ">v >v >v >v >v >v >v >v >v >v >v v\n",
      ">v >v >v >v >v >v >v >v >v >v >v v\n",
      "> > > > > > > > > > > v\n",
      "^ T T T T T T T T T T T\n",
      "[0.25, 0.25, 0.25, 0.25]\n",
      "\n"
     ]
    }
   ],
   "execution_count": 21
  },
  {
   "cell_type": "markdown",
   "id": "4e865218",
   "metadata": {},
   "source": [
    "## 4. Matrix–vector Representation of DP\n",
    "\n",
    "We’ll use a small finite MDP ($|X|=16$, $|A|=4$) and **matrix** forms to compute the optimal $Q$-function:\n",
    "\n",
    "4.1. Build the **transition matrix**\n",
    "   $\n",
    "   P \\in \\mathbb{R}^{|X||A|\\times |X|}\n",
    "   $\n",
    "   and the **immediate reward** vector\n",
    "   $\n",
    "   r \\in \\mathbb{R}^{|X||A|}.\n",
    "   $\n",
    "\n",
    "4.2. Using the matrix form of the $Q$-value function $Q_\\pi$ and the value function $V_\\pi$ to write down the bellman equation.\n",
    "\n",
    "4.3. Define the **Bellman optimality operator**:\n",
    "   $$\n",
    "   T^\\star(Q) = r + \\gamma\\, P\\, J_Q,\n",
    "   $$\n",
    "   where\n",
    "   $$\n",
    "   (J_Q)(x) = \\max_{a} Q(x,a).\n",
    "   $$\n",
    "   Iterating $Q_{k+1} = T^\\star(Q_k)$ converges to the optimal $Q^\\star$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35440cec",
   "metadata": {},
   "source": [
    "### 4×4 Gridworld — From Bottom‑Left (Start) to Top‑Right (Goal)\n",
    "\n",
    "**States:** 16 cells in a 4×4 grid, row-major indexing with top-left as (row=0, col=0).\n",
    "State id: `s = row * 4 + col`, rows increase downward.\n",
    "\n",
    "**Start:** bottom-left `(row=3, col=0)` → `s_start = 12`\n",
    "**Goal:** top-right `(row=0, col=3)` → `s_goal = 3`\n",
    "\n",
    "**Actions (4):**\n",
    "- `a=0` → UP (↑)\n",
    "- `a=1` → RIGHT (→)\n",
    "- `a=2` → DOWN (↓)\n",
    "- `a=3` → LEFT (←)\n",
    "\n",
    "**Dynamics:** Deterministic. If an action would leave the grid world, the agent stays in place.\n",
    "\n",
    "**Rewards (maximize):**\n",
    "- `-1` per step\n",
    "- `0` in the goal\n",
    "\n",
    "**Terminal:** The goal is absorbing (from goal, any action keeps you at goal with reward 0)."
   ]
  },
  {
   "cell_type": "code",
   "id": "76ef97dd",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-26T23:01:16.579346Z",
     "start_time": "2025-09-26T23:01:16.575651Z"
    }
   },
   "source": [
    "# Grid size\n",
    "nrow, ncol = 4, 4\n",
    "nS = nrow * ncol  # |X| = 16\n",
    "nA = 4  # |A| = 4 (UP, RIGHT, DOWN, LEFT)\n",
    "\n",
    "# Start (bottom-left) and Goal (top-right)\n",
    "s_start = (nrow - 1) * ncol + 0  # 12\n",
    "s_goal  = 0 * ncol + (ncol - 1)  # 3\n",
    "\n",
    "# Row-major state id\n",
    "def s_id(r, c):\n",
    "    return r * ncol + c\n",
    "\n",
    "# For state-action row index in matrices of shape (nS*nA, ...)\n",
    "def sa_id(s, a):\n",
    "    return s * nA + a\n",
    "\n",
    "# Action deltas: 0=UP, 1=RIGHT, 2=DOWN, 3=LEFT\n",
    "DELTAS = {\n",
    "    0: (-1,  0),  # UP:    row-1\n",
    "    1: ( 0,  1),  # RIGHT: col+1\n",
    "    2: ( 1,  0),  # DOWN:  row+1\n",
    "    3: ( 0, -1),  # LEFT:  col-1\n",
    "}\n",
    "\n",
    "# Quick sanity checks and a tiny ASCII map\n",
    "print(\"Grid 4×4. Start=S (row=3,col=0), Goal=G (row=0,col=3)\")\n",
    "for rrow in range(nrow):\n",
    "    line = []\n",
    "    for ccol in range(ncol):\n",
    "        s = s_id(rrow, ccol)\n",
    "        if s == s_start:\n",
    "            line.append(\" S \")\n",
    "        elif s == s_goal:\n",
    "            line.append(\" G \")\n",
    "        else:\n",
    "            line.append(f\"{s:2d}\")\n",
    "    print(\" \".join(line))\n",
    "    "
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Grid 4×4. Start=S (row=3,col=0), Goal=G (row=0,col=3)\n",
      " 0  1  2  G \n",
      " 4  5  6  7\n",
      " 8  9 10 11\n",
      " S  13 14 15\n"
     ]
    }
   ],
   "execution_count": 24
  },
  {
   "cell_type": "markdown",
   "id": "94e2d4d6",
   "metadata": {},
   "source": [
    "### 4.1 Build Transition Matrix and Reward Vector\n",
    "\n",
    "**Definition:**\n",
    "\n",
    "- **Transition matrix** $P \\in \\mathbb{R}^{|X||A|\\times |X|}$\n",
    "  Rows index state–action pairs $(x,a)$, columns index next states $x'$.\n",
    "  Entry:\n",
    "  $$\n",
    "  P[(x,a),\\,x'] \\;\\equiv\\; \\Pr\\{X_{t+1}=x' \\mid X_t=x,\\; A_t=a\\}.\n",
    "  $$\n",
    "  Row-wise normalization holds: $\\sum_{x'} P[(x,a),x'] = 1$ for every $(x,a)$.\n",
    "\n",
    "- **Reward vector** $r \\in \\mathbb{R}^{|X||A|}$ (reward maximization form)\n",
    "  Each entry is the one-step expected reward under $(x,a)$:\n",
    "  $$\n",
    "  r[(x,a)] \\;\\equiv\\; \\mathbb{E}\\big[\\,R_{t+1}\\mid X_t=x,\\; A_t=a\\,\\big].\n",
    "  $$\n",
    "\n",
    "**Indexing note.**\n",
    "A convenient index for $(x,a)$ is\n",
    "$$\n",
    "i = x\\,|A| + a\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "id": "86995a5d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-26T23:02:07.400382Z",
     "start_time": "2025-09-26T23:02:07.396982Z"
    }
   },
   "source": [
    "\n",
    "# Build P (|X||A| × |X|) and r (|X||A|)\n",
    "P = np.zeros((nS * nA, nS), dtype=float)\n",
    "r = np.zeros(nS * nA, dtype=float)\n",
    "\n",
    "for s in range(nS):\n",
    "# This will give // and %\n",
    "    r0, c0 = divmod(s, ncol)\n",
    "\n",
    "    for a in range(nA):\n",
    "        _s_id = sa_id(s, a)\n",
    "\n",
    "        # Goal is absorbing with reward 0\n",
    "        if s == s_goal:\n",
    "            P[_s_id, s_goal] = 1.0\n",
    "            r[_s_id] = 0\n",
    "            continue\n",
    "\n",
    "        dr, dc = DELTAS[a]\n",
    "        rr = min(nrow - 1, max(0, r0 + dr))\n",
    "        cc = min(ncol - 1, max(0, c0 + dc))\n",
    "        s_next = s_id(rr, cc)\n",
    "\n",
    "        # Deterministic transition\n",
    "        P[_s_id, s_next] = 1.0\n",
    "\n",
    "        # Reward: -1 per step, 0 in goal (already handled above)\n",
    "        r[_s_id] = -1\n"
   ],
   "outputs": [],
   "execution_count": 27
  },
  {
   "cell_type": "markdown",
   "id": "8d5a23e0",
   "metadata": {},
   "source": [
    "### 4.2 Matrix Form of Bellman Consistency and Bellman equation\n",
    "\n",
    "Q-evaluation when a fixed policy $\\pi$ is given:\n",
    "\n",
    "$$\n",
    "Q_\\pi(x,a) = r(x,a) + \\gamma \\,\\mathbb{E}_{x'\\sim P(\\cdot \\mid x,a)} \\, V_\\pi(x') \\tag{4.3(1)}\n",
    "$$\n",
    "\n",
    "The bellman equation:\n",
    "$$\n",
    "Q^\\star(x,a) \\;=\\; r(x,a) \\;+\\; \\gamma \\, \\mathbb{E}_{x' \\sim P(\\cdot \\mid x,a)}\n",
    "\\left\\{ \\max_{a' \\in A} Q^\\star(x',a') \\right\\},\n",
    "\\qquad \\forall (x,a) \\in X \\times A. \\tag{4.3(2)}\n",
    "$$\n",
    "\n",
    "where $Q^\\star$ is the optimal $Q$-value function. Similarly, let us define\n",
    "\n",
    "$$\n",
    "J_Q(x) = \\max_{a \\in A} Q(x,a).\n",
    "$$\n",
    "\n",
    "Question: How to write these equations (4.3(1))&(2) in matrix and operator form?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "570aac90",
   "metadata": {},
   "source": [
    "**(TODO) Answer:**\n"
   ]
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "---\n",
    "**Answer: 4.2**\n",
    "\n",
    "Starting point:\n",
    "\n",
    "$$\n",
    "Q_\\pi(x,a) = r(x,a) + \\gamma ,\\mathbb{E}{x’\\sim P(\\cdot \\mid x,a)} , V\\pi(x’),\n",
    "\\qquad V_\\pi(x) = \\sum_a \\pi(a\\mid x), Q_\\pi(x,a).\n",
    "$$\n",
    "\n",
    "Stacked into vector–matrix form:\n",
    "\n",
    "$$\n",
    "Q_\\pi = r + \\gamma , P , V_\\pi,\n",
    "\\qquad V_\\pi = \\Pi Q_\\pi,\n",
    "$$\n",
    "\n",
    "where\n",
    "- $Q_\\pi \\in \\mathbb{R}^{|X||A|}$\n",
    "- $\\in \\mathbb{R}^{|X||A|}$\n",
    "- $\\in \\mathbb{R}^{|X||A| \\times |X|}$\n",
    "- $i \\in \\mathbb{R}^{|X| \\times |X||A|} with \\Pi[x,(x,a)]=\\pi(a\\mid x)$\n",
    "\n",
    "Eliminating $V_\\pi$:\n",
    "\n",
    "$$\n",
    "Q_\\pi = r + \\gamma , (P \\Pi), Q_\\pi.\n",
    "$$\n",
    "\n",
    "So equivalently:\n",
    "\n",
    "$$\n",
    "(\\mathbf{I} - \\gamma P \\Pi), Q_\\pi = r,\n",
    "\\qquad\n",
    "Q_\\pi = (\\mathbf{I} - \\gamma P \\Pi)^{-1} r.\n",
    "$$\n",
    "\n",
    "In state-value form:\n",
    "\n",
    "$$\n",
    "V_\\pi = R^\\pi + \\gamma P^\\pi V_\\pi,\n",
    "\\qquad\n",
    "R^\\pi := \\Pi r, \\quad P^\\pi := \\Pi P,\n",
    "$$\n",
    "\n",
    "so\n",
    "\n",
    "$$\n",
    "V_\\pi = (\\mathbf{I} - \\gamma P^\\pi)^{-1} R^\\pi,\n",
    "\\qquad\n",
    "Q_\\pi = r + \\gamma P V_\\pi.\n",
    "$$\n",
    "\n",
    "Define the policy Bellman operator:\n",
    "\n",
    "$$\n",
    "(T^\\pi Q) = r + \\gamma, P, (\\Pi Q).\n",
    "$$\n",
    "\n",
    "Then $Q_\\pi$ is the unique fixed point:\n",
    "\n",
    "$$\n",
    "Q_\\pi = T^\\pi Q_\\pi.\n",
    "$$\n",
    "\n",
    "From the optimality condition:\n",
    "\n",
    "$$\n",
    "Q^\\star(x,a) = r(x,a) + \\gamma ,\\mathbb{E}{x’ \\sim P(\\cdot \\mid x,a)} \\Big[ \\max{a’} Q^\\star(x’,a’) \\Big].\n",
    "$$\n",
    "\n",
    "Define the nonlinear operator:\n",
    "\n",
    "$$\n",
    "(T^\\star Q) = r + \\gamma, P, J_Q,\n",
    "\\qquad\n",
    "(J_Q)(x) := \\max_{a} Q(x,a).\n",
    "$$\n",
    "\n",
    "Then the optimal $Q^\\star$ is the unique fixed point:\n",
    "\n",
    "$$\n",
    "Q^\\star = T^\\star(Q^\\star).\n",
    "$$\n",
    "\n",
    "And the optimal state-value and greedy policy follow:\n",
    "\n",
    "$$\n",
    "V^\\star(x) = \\max_a Q^\\star(x,a),\n",
    "\\qquad\n",
    "\\pi^\\star(x) \\in \\arg\\max_a Q^\\star(x,a).\n",
    "$$\n",
    "\n",
    "---"
   ],
   "id": "ff96c3077d3b471a"
  },
  {
   "cell_type": "markdown",
   "id": "054a258d",
   "metadata": {},
   "source": [
    "### 4.3 Solve bellman equation.\n",
    "\n",
    "Note that $J_Q$ has dimension $|X|$. With these notations, the *Bellman optimality operator* is defined as\n",
    "\n",
    "$$\n",
    "T^\\star Q \\;=\\; g + \\gamma P J_Q,\n",
    "\\tag{2.27}\n",
    "$$\n",
    "\n",
    "which is nothing but a matrix representation of the right-hand side of Bellman equation.\n",
    "This allows us to concisely write the Bellman equation as\n",
    "\n",
    "$$\n",
    "Q = T^\\star Q.\n",
    "\\tag{2.28}\n",
    "$$\n",
    "\n",
    "One can do to solve this equation is through *fix-point iteration*:\n",
    "$$\n",
    "Q_{n+1} = T^\\star Q_n.\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "id": "1b44ffe2",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-27T01:14:38.781554Z",
     "start_time": "2025-09-27T01:14:38.777929Z"
    }
   },
   "source": [
    "Q = np.zeros((nS * nA), dtype=float)\n",
    "for i in range(1000):\n",
    "    old_Q = Q.copy()\n",
    "    J_Q = Q.reshape(nS, nA).max(axis=1)\n",
    "    Q = r + gamma * (P @ J_Q)\n",
    "    if np.max(np.abs(Q - old_Q)) < 1e-10:\n",
    "        print(f\"Converged in {i+1} iterations.\")\n",
    "        break\n",
    "J_Q = Q.reshape(nS, nA).max(axis=1)    \n",
    "print(\"\\nOptimal state values J_Q (V*) on the grid:\")\n",
    "for r0 in range(nrow):\n",
    "    row_vals = []\n",
    "    for c0 in range(ncol):\n",
    "        s = r0 * ncol + c0\n",
    "        if s == s_start:\n",
    "            row_vals.append(\" S \")\n",
    "        elif s == s_goal:\n",
    "            row_vals.append(\" G \")\n",
    "        else:\n",
    "            row_vals.append(f\"{J_Q[s]:6.2f}\")\n",
    "    print(\" \".join(row_vals))"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converged in 8 iterations.\n",
      "\n",
      "Optimal state values J_Q (V*) on the grid:\n",
      " -2.85  -1.95  -1.00  G \n",
      " -3.71  -2.85  -1.95  -1.00\n",
      " -4.52  -3.71  -2.85  -1.95\n",
      " S   -4.52  -3.71  -2.85\n"
     ]
    }
   ],
   "execution_count": 29
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "from vuer import vuer\n",
    "from vuer import robots\n",
    "\n",
    "humanoid = robots.humanoid.make()\n",
    "world = vuer.make()"
   ],
   "id": "110f31d6be2f90a4"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "2025ocrl-pset",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

<html>
<head>
<title>pset1.ipynb</title>
<meta http-equiv="Content-Type" content="text/html; charset=utf-8">
<style type="text/css">
.s0 { color: #7a7e85;}
.s1 { color: #bcbec4;}
.s2 { color: #cf8e6d;}
.s3 { color: #bcbec4;}
.s4 { color: #2aacb8;}
.s5 { color: #6aab73;}
.s6 { color: #5f826b; font-style: italic;}
.ls0 { height: 1px; border-width: 0; color: #43454a; background-color:#43454a}
</style>
</head>
<body bgcolor="#1e1f22">
<table CELLSPACING=0 CELLPADDING=5 COLS=1 WIDTH="100%" BGCOLOR="#606060" >
<tr><td><center>
<font face="Arial, Helvetica" color="#000000">
pset1.ipynb</font>
</center></td></tr></table>
<pre><span class="s0">#%% md 
</span><span class="s1"># Problem Set 1 — Four Problems in Dynamic Programming 
 
*Markov Decision Processes &amp; DP Methods* 
 
**Due: September 26 (Friday), 11:59pm** 
 
### Problem 1 — Inscribed Polygon of Maximum Perimeter (Pen &amp; Paper) 
TODO list: 
- (a) show Q-value function $Q_{N-1}$ (6pt) 
- (b) show convexity (6pt) 
- (c) show optimal control signal $u_{N-1}$ (6pt) 
- (d) induction to any $k$-th step Q-function $Q_k$ (6pt) 
- (e) show all optimal control signal $u_k$ (6pt) 
 
Bonus: 
- (f) show convexity (5pt) 
- (g)(coding) solve the problem using optimization (5pt) 
### Problem 2 — Proof of convergence of value iteration (Pen &amp; Paper) 
TODO list: 
- 2.1 contraction of bellman operator (5pt) 
- 2.2 linear convergence (5pt) 
- 2.3 stoping criteria (5pt) 
- 2.4 iteration bound (5pt) 
### Problem 3 — Cliffwalk (coding) 
TODO list: 
- 3.2 fill in code for policy evaluation (10pt) 
- 3.3 fill in code for policy iteration (10pt) 
- 3.4 fill in code for value iteration (10pt) 
 
### Problem 4 — Matrix–Vector Representation of DP 
TODO list: 
- 4.1 build the transition matrix $P$ (5pt) 
- 4.2 write bellman equation as matrix form (5pt) 
- 4.3 solve the matrix equation by fix-point iteration (10pt) <hr class="ls0"></span><span class="s0">#%% md 
</span><span class="s1">## 1. Inscribed Polygon of Maximal Perimeter (Pen and Paper) <hr class="ls0"></span><span class="s0">#%% md 
</span><span class="s1">In lectures, we have seen how dynamic programming (DP) can compute optimal value functions and optimal policies for finite-horizon MDPs with discrete state space and action space (i.e., the tabular case). 
 
In this exercise, we will see that DP can also solve an optimal control problem with continuous state space and action space. 
This problem is a geometry problem where we try to find the $N$-side polygon inscribed inside a circle with maximum perimeter. We will walk you through the key steps of formulating and solving the problem, while leaving a few mathematical details for you to fill in. 
 
Given a circle with radius $1$, we can randomly choose $N$ distinct points on the circle to form a polygon with $N$ vertices and sides, as shown in Fig. 1 with $N=3,4,5$. 
 
&lt;figure style=&quot;text-align:center;&quot;&gt; 
  &lt;img src=&quot;https://raw.githubusercontent.com/ComputationalRobotics/2025-ES-AM-158-PSET/main/PSET1/polygon-inside-circle.png&quot; width=&quot;600&quot; alt=&quot;Inscribed polygon&quot;&gt; 
  &lt;figcaption style=&quot;color:#6a737d; font-style:italic;&quot;&gt; 
    Figure 1. Polygons inscribed inside a circle 
  &lt;/figcaption&gt; 
&lt;/figure&gt; 
 
Once the $N$ points are chosen, the $N$-polygon will have a perimeter, i.e., the sum of the lengths of its edges. 
 
What is the configuration of the $N$ points such that the resulting $N$-polygon has the maximum perimeter? We claim that the answer is when the $N$-polygon has edges of equal lengths, or in other words, when the $N$ points are placed on the circle evenly. 
 
Let us use dynamic programming to prove the claim. 
 
To use dynamic programming, we need to define a dynamical system and a reward function. 
 
&lt;figure style=&quot;text-align:center;&quot;&gt; 
  &lt;img src=&quot;https://raw.githubusercontent.com/ComputationalRobotics/2025-ES-AM-158-PSET/main/PSET1/sequential-placement-N-point.png&quot; width=&quot;360&quot; alt=&quot;Inscribed polygon&quot;&gt; 
  &lt;figcaption style=&quot;color:#6a737d; font-style:italic;&quot;&gt; 
    Figure 2. Sequential placement of N points on the circle. 
  &lt;/figcaption&gt; 
&lt;/figure&gt; 
 
**Dynamical system.** 
 
 
We will use $\{x_1, \ldots, x_N\}$ to denote the angular positions of the $N$ points to be placed on the circle (with slight abuse of notation, we will call each of those points $x_k$ as well). In particular, as shown in Fig. 2, let us use $x_k$ to denote the angle between the line $O — x_k$ and the vertical line (O is the center of the circle), with zero angle starting at 12 o’clock and clockwise being positive. Without loss of generality, we assume $x_1 = 0$. (if $x_1$ is nonzero, we can always rotate the entire circle so that $x_1 = 0$). 
 
After the $k$-th point is placed, we can “control” where the next point $x_{k+1}$ will be, by deciding the incremental angle between $x_{k+1}$ and $x_k$, denoted as $u_k &gt; 0$ in Fig. 2. This is simply saying the dynamics is 
 
$$ 
x_{k+1} = x_k + u_k, \quad k=1,\ldots,N-1, \quad x_1 = 0. 
$$ 
 
Notice here we did not use an MDP to formulate this problem because the dynamics is deterministic. In the MDP language, this would correspond to, at time step $k$, if the agent takes action $u_k$ at state $x_k$, then the probability of transitioning to state $x_k + u_k$ at time $k+1$ is $1$, and the probability of transitioning to other states is zero. 
 
 
**Reward.** 
 
 
The perimeter of the $N$-polygon is therefore 
 
$$ 
g_N(x_N) + \sum_{k=1}^{N-1} g_k(x_k,u_k), 
$$ 
 
with the terminal reward 
 
$$ 
g_N(x_N) = 2 \sin\left(\frac{2\pi-x_N}{2}\right), 
$$ 
 
the distance between $x_N$ and $x_1$ (see Fig. 2), and the running reward 
 
$$ 
g_k(x_k,u_k) = 2 \sin\left(\frac{u_k}{2}\right). 
$$ 
 
**Dynamic programming.** 
 
We are now ready to invoke dynamic programming. Recall in lectures the key steps of DP are first to initialize the optimal value functions at the terminal time $k=N$, and then perform backward recursion to compute the optimal value functions at time $k=N-1,\dots,1$. 
 
We start by setting 
 
$$ 
V_N(x_N) = g_N(x_N) = 2 \sin\left(\frac{2\pi-x_N}{2}\right). 
$$ 
 
Unlike in lectures where we initialized the terminal value functions as all zero, here we initialize the terminal value functions as $g_N(x_N)$ because there is a &quot;terminal-state&quot; reward. 
 
We then compute $V_{N-1}(x_{N-1})$ as 
 
$$ 
V_{N-1}(x_{N-1}) 
= \max_{0 &lt; u_{N-1} &lt; 2\pi-x_{N-1}} 
\left\{ 
  \underbrace{ 2 \sin\left(\tfrac{u_{N-1}}{2}\right) + V_N(x_{N-1} + u_{N-1}) }_{Q_{N-1}(x_{N-1}, u_{N-1})} 
\right\}, 
\tag{9.1} 
$$ 
 
where $u_{N-1} &lt; 2\pi-x_{N-1}$ because we do not want $x_N$ to cross $2\pi$. <hr class="ls0"></span><span class="s0">#%% md 
</span>

<span class="s1">**a**. Show that 
 
$$ 
Q_{N-1}(x_{N-1}, u_{N-1}) 
= 2 \sin\left(\tfrac{u_{N-1}}{2}\right) 
+ 2 \sin\left(\tfrac{2\pi-x_{N-1}-u_{N-1}}{2}\right), 
$$ 
 
and thus 
 
$$ 
\frac{\partial Q_{N-1}(x_{N-1}, u_{N-1})}{\partial u_{N-1}} 
= \cos\left(\tfrac{u_{N-1}}{2}\right) 
- \cos\left(\tfrac{2\pi-x_{N-1}-u_{N-1}}{2}\right). 
$$ 
 
**(TODO) ANSWER:** 
 <hr class="ls0"></span><span class="s0">#%% md 
</span><span class="s1">--- 
**Answer: 1.a** 
 
From class, we derived that the Bellman optimality equation for $Q^*$ is 
$$ 
Q^*(x_{N-1}, u_{N-1}) = \sum_{x_{N}, r} P(x_{N}, r \mid x_{N-1}, u_{N-1})\,\Big[ r + \gamma \max_{u_{N}} Q^*(x_{N}, u_{N}) \Big], 
$$ 
where $r$ is the immediate reward. 
 
Intuitively, this says that the optimal action-value of $(x_{N-1}, u_{N-1})$, the **penultimate state**, is the immediate reward plus the discounted value of the best action in the final step. 
 
Within the context of this problem, we know that the transition dynamics are deterministic. Likewise, we can set the discount factor $\gamma = 1$, as this is given to be a finite-horizon MDP. Thus, we can simplify: 
 
$$ 
Q^*(x_{N-1}, u_{N-1}) = \sum_{x_{N}, r} P(x_{N}, r \mid x_{N-1}, u_{N-1})\,\Big[ r + \max_{u_{N}} Q^*(x_{N}, u_{N}) \Big], 
$$ 
 
so that 
$$ 
Q^*(s, a) = r + \max_{u_{N}} Q^*(x_{N}, u_{N}). 
$$ 
 
In addition, we are given the reward, which is the additional length of every $u_{N-1}$: 
 
$$ 
r = 2 \sin\!\left(\tfrac{u_{N-1}}{2}\right). 
$$ 
 
Now, we need to make sense of $\max_{u_{N}} Q^*(x_{N}, u_{N})$, which is the optimal value of the **final state** $u_{N}$. At this last step, we are bounded between $x_{N-1} + u_{N-1}$ and $2\pi$. This means the reward of the final step is 
 
$$ 
\max_{u_{N}} Q^*(x_{N}, u_{N}) = 2 \sin\!\left(\tfrac{2\pi - x_{N-1} - u_{N-1}}{2}\right). 
$$ 
 
Putting it all together, we obtain 
 
$$ 
Q^*(x_{N-1}, u_{N-1}) = 2 \sin\!\left(\tfrac{u_{N-1}}{2}\right) + 2 \sin\!\left(\tfrac{2\pi - x_{N-1} - u_{N-1}}{2}\right). 
$$ 
 
When we take the partial derivative of $Q^*(x_{N-1}, u_{N-1})$ with respect to $u_{N-1}$, we arrive at the following: 
 
$$ 
\frac{\partial Q^*}{\partial u_{N-1}} 
= 2\cos\!\Big(\tfrac{u_{N-1}}{2}\Big)\cdot\tfrac{1}{2} 
+ 2\cos\!\Big(\tfrac{2\pi - x_{N-1} - u_{N-1}}{2}\Big)\cdot\frac{\partial}{\partial u_{N-1}}\!\Big(\tfrac{2\pi - x_{N-1} - u_{N-1}}{2}\Big), 
$$ 
 
which simplifies to 
 
$$ 
\frac{\partial Q^*}{\partial u_{N-1}} 
=\cos\!\Big(\tfrac{u_{N-1}}{2}\Big)\;-\;\cos\!\Big(\tfrac{2\pi - x_{N-1} - u_{N-1}}{2}\Big). 
$$ 
 
--- <hr class="ls0"></span><span class="s0">#%% md 
</span>

<span class="s1">**b**. Show that $Q_{N-1}(x_{N-1}, u_{N-1})$ is concave (i.e., $-Q_{N-1}(x_{N-1}, u_{N-1})$ is convex) in $u_{N-1}$ for every $x_{N-1} \in (0, \pi)$ and $u_{N-1} \in (0, 2\pi-x_{N-1})$. 
(Hint: compute the second derivative of $Q_{N-1}(x_{N-1}, u_{N-1})$ with respect to $u_{N-1}$ and verify it is positive definite) 
 
**(TODO) ANSWER:** <hr class="ls0"></span><span class="s0">#%% md 
</span><span class="s1">--- 
**Answer: 1.b** 
 
We start with 
$$ 
Q_{N-1}(x_{N-1}, u_{N-1}) 
= 2 \sin\!\left(\tfrac{u_{N-1}}{2}\right) 
+ 2 \sin\!\left(\tfrac{2\pi - x_{N-1} - u_{N-1}}{2}\right). 
$$ 
 
Taking the first derivative with respect to $u_{N-1}$: 
$$ 
\frac{\partial Q_{N-1}}{\partial u_{N-1}} 
= \cos\!\Big(\tfrac{u_{N-1}}{2}\Big) 
- \cos\!\Big(\tfrac{2\pi - x_{N-1} - u_{N-1}}{2}\Big). 
$$ 
 
Taking the second derivative: 
$$ 
\frac{\partial^2 Q_{N-1}}{\partial u_{N-1}^2} 
= -\tfrac{1}{2}\sin\!\Big(\tfrac{u_{N-1}}{2}\Big) 
- \tfrac{1}{2}\sin\!\Big(\tfrac{2\pi - x_{N-1} - u_{N-1}}{2}\Big). 
$$ 
 
Now note that on the domain 
$$ 
x_{N-1} \in (0,\pi), \qquad u_{N-1} \in (0,\,2\pi - x_{N-1}), 
$$ 
we have 
$$ 
\tfrac{u_{N-1}}{2} \in (0,\pi), 
\qquad \tfrac{2\pi - x_{N-1} - u_{N-1}}{2} \in (0,\pi). 
$$ 
Thus both sine terms are strictly positive, implying 
$$ 
\frac{\partial^2 Q_{N-1}}{\partial u_{N-1}^2} &lt; 0 
\quad\text{throughout the domain}. 
$$ 
 
Therefore, $Q_{N-1}(x_{N-1}, u_{N-1})$ is **strictly concave** in $u_{N-1}$ for all $x_{N-1} \in (0,\pi)$ and $u_{N-1} \in (0,\,2\pi - x_{N-1})$. 
 
--- <hr class="ls0"></span><span class="s0">#%% md 
</span>

<span class="s1">**c**. With a and b, show that the optimal $u_{N-1}$ that solves (9.1) is 
 
$$ 
u_{N-1}^\star = \frac{2\pi-x_{N-1}}{2}, 
$$ 
 
and therefore 
 
$$ 
J_{N-1}(x_{N-1}) = 4 \sin\left(\tfrac{2\pi-x_{N-1}}{4}\right). 
$$ 
 
(Hint: the point at which a concave function’s gradient vanishes must be the unique maximizer of that function.) 
 
**(TODO) ANSWER:** <hr class="ls0"></span><span class="s0">#%% md 
</span><span class="s1">--- 
**Answer: 1.c** 
 
Because $Q^*$ is strictly concave, any stationary point is the **unique** maximizer. Setting the first derivative to zero, 
$$ 
\cos\!\Big(\tfrac{u_{N-1}}{2}\Big) 
=\cos\!\Big(\tfrac{2\pi - x_{N-1} - u_{N-1}}{2}\Big) 
\;\Rightarrow\; 
u_{N-1}^*=\frac{2\pi - x_{N-1}}{2}, 
$$ 
which lies in $\big(0,\,2\pi-x_{N-1}\big)$. Thus, the unique optimal split at step $N-1$ is to take **half of the remaining angle**; the final step $u_N^*$ takes the other half. 
 
Knowing this, we can plug back into our state-action value function to find $J_{N-1}(x_{N-1})$: 
 
$$ 
J_{N-1}(x_{N-1}) = Q_{N-1}(x_{N-1}, u_{N-1}^*) = 4 \sin\!\Big(\tfrac{2\pi - x_{N-1}}{4}\Big). 
$$ 
 
--- <hr class="ls0"></span><span class="s0">#%% md 
</span>

<span class="s1">**d**. Now use induction to show that the $k$-th step dynamic programming 
 
$$ 
J_k(x_k) 
= \max_{0 &lt; u_k &lt; 2\pi — x_k} 
\left\{ 2 \sin\left(\tfrac{u_k}{2}\right) + J_{k+1}(x_k + u_k) \right\} 
$$ 
 
admits an optimal control 
 
$$ 
u_k^\star = \frac{2\pi-x_k}{N-k + 1}, 
$$ 
 
and optimal cost-to-go 
 
$$ 
J_k(x_k) = 2 (N-k + 1) \, \sin\!\left( \frac{2\pi-x_k}{2 (N-k + 1)} \right). 
$$ 
 
**(TODO) ANSWER:** <hr class="ls0"></span><span class="s0">#%% md 
</span><span class="s1">--- 
**Answer: 1.d** 
 
We prove the claim by induction on the step index $k$. 
 
**Base Case ($k = N-1$).** 
From part (c), we showed that 
$$ 
u_{N-1}^\star = \frac{2\pi - x_{N-1}}{2}, \qquad 
J_{N-1}(x_{N-1}) = 4 \sin\!\left(\frac{2\pi - x_{N-1}}{4}\right). 
$$ 
This matches the proposed formula with $N-k+1 = 2$: 
$$ 
u_{N-1}^\star = \frac{2\pi - x_{N-1}}{2}, \qquad 
J_{N-1}(x_{N-1}) = 2 (2) \sin\!\left(\frac{2\pi - x_{N-1}}{2 \cdot 2}\right). 
$$ 
So the base case holds. 
 
**Inductive Step.** 
Assume the formula holds for step $k+1$: 
$$ 
u_{k+1}^\star = \frac{2\pi - x_{k+1}}{N-k}, \qquad 
J_{k+1}(x_{k+1}) = 2 (N-k)\,\sin\!\left(\frac{2\pi - x_{k+1}}{2 (N-k)}\right). 
$$ 
 
We need to show it also holds for step $k$. 
 
By definition of dynamic programming, 
$$ 
J_k(x_k) = \max_{0 &lt; u_k &lt; 2\pi - x_k} 
\left\{ 2 \sin\!\left(\tfrac{u_k}{2}\right) + J_{k+1}(x_k + u_k) \right\}. 
$$ 
 
Substitute the inductive hypothesis for $J_{k+1}$: 
$$ 
J_k(x_k) = \max_{0 &lt; u_k &lt; 2\pi - x_k} 
\left\{ 2 \sin\!\left(\tfrac{u_k}{2}\right) 
+ 2 (N-k)\,\sin\!\Bigg(\frac{2\pi - (x_k + u_k)}{2 (N-k)}\Bigg) \right\}. 
$$ 
 
**Concavity Argument.** 
The objective inside the braces is strictly concave in $u_k$, so the maximizer is unique. 
Setting the first derivative with respect to $u_k$ equal to zero yields 
$$ 
\cos\!\left(\tfrac{u_k}{2}\right) 
= \cos\!\left(\frac{2\pi - x_k - u_k}{2 (N-k)}\right). 
$$ 
The feasible solution is 
$$ 
u_k^\star = \frac{2\pi - x_k}{N-k+1}. 
$$ 
 
**Compute the optimal cost.** 
Plugging $u_k^\star$ back into the expression for $J_k$: 
 
$$ 
\begin{aligned} 
J_k(x_k) &amp;= 2 \sin\!\left(\frac{u_k^\star}{2}\right) 
+ 2 (N-k)\,\sin\!\left(\frac{2\pi - x_k - u_k^\star}{2 (N-k)}\right). 
\end{aligned} 
$$ 
 
Using $u_k^\star = \tfrac{2\pi - x_k}{N-k+1}$, both sine arguments simplify to the same value: 
$$ 
\frac{u_k^\star}{2} = \frac{2\pi - x_k}{2 (N-k+1)}, 
\qquad 
\frac{2\pi - x_k - u_k^\star}{2 (N-k)} 
= \frac{2\pi - x_k}{2 (N-k+1)}. 
$$ 
 
Therefore, 
$$ 
J_k(x_k) = 2 \sin\!\left(\frac{2\pi - x_k}{2 (N-k+1)}\right) 
+ 2 (N-k) \sin\!\left(\frac{2\pi - x_k}{2 (N-k+1)}\right) 
$$ 
 
Simplifying, 
$$ 
J_k(x_k) = 2 (N-k+1) \sin\!\left(\frac{2\pi - x_k}{2 (N-k+1)}\right). 
$$ 
 
**Conclusion.** 
By induction, for all $k=0,1,\dots,N-1$, the optimal policy and cost-to-go are 
$$ 
u_k^\star = \frac{2\pi - x_k}{N-k+1}, 
\qquad 
J_k(x_k) = 2 (N-k+1) \sin\!\left(\frac{2\pi - x_k}{2 (N-k+1)}\right). 
$$ 
 
--- 
 <hr class="ls0"></span><span class="s0">#%% md 
</span>

<span class="s1">**e**. Starting from $x_1 = 0$, what is the optimal sequence of controls? 
 
Hopefully now you see why my original claim is true! 
 
**(TODO) ANSWER:** <hr class="ls0"></span><span class="s0">#%% md 
</span><span class="s1">--- 
**Answer: 1.e** 
 
The optimal control sequence is to make each split of equal size. 
 
At k=1: 
$$ 
u_1^\star = \frac{2\pi - x_1}{N} = \frac{2\pi}{N}. 
$$ 
So the first control is $2\pi/N$ 
Then $x_2 = x_1 + u_1^\star = 2\pi/N$. 
Plugging into the formula: 
$$ 
u_2^\star = \frac{2\pi - x_2}{N-1} 
= \frac{2\pi - 2\pi/N}{N-1} 
= \frac{2\pi}{N}. 
$$ 
Same again. 
Repeating this, we find that 
$$ 
u_k^\star = \frac{2\pi}{N} \quad \text{for all } k=1,\dots,N. 
$$ 
 
--- <hr class="ls0"></span><span class="s0">#%% md 
</span><span class="s1">### Bonus: 
We are not yet done for this exercise. Since you have probably already spent quite some time on this exercise, I will leave the rest of the exercise a bonus. In case you found this simple geometric problem interesting, you should keep reading as we will use numerical techniques to prove the same claim. 
 
In Fig. 2, by denoting 
 
$$ 
u_N = 2\pi - x_N = 2\pi - (u_1 + \cdots + u_{N-1}) 
$$ 
 
as the angle between the line $O — x_N$ and the line $O — x_1$, it is not hard to observe that the perimeter of the $N$-polygon is 
 
$$ 
\sum_{k=1}^N 2 \sin\!\left(\tfrac{u_k}{2}\right). 
$$ 
 
Consequently, to maximize the perimeter, we can formulate the following optimization 
 
$$ 
\max_{u_1,\ldots,u_N} \;\; \sum_{k=1}^N 2 \sin\!\left(\tfrac{u_k}{2}\right) 
$$ 
 
subject to 
 
$$ 
u_k &gt; 0, \; k = 1, \ldots, N, \\ 
u_1 + \cdots + u_N = 2\pi 
\tag{9.2} 
$$ 
 
where $u_k$ can be seen as the angle spanned by the line $x_k — x_{k+1}$ with respect to the center $O$ so that they are positive and sum up to $2\pi$. <hr class="ls0"></span><span class="s0">#%% md 
</span>

<span class="s1">**f**. Show that the optimization (9.2) is convex. 
(Hint: first show the feasible set is convex, and then show the objective function is concave over the feasible set.) 
 
**(TODO) ANSWER:** 
 
Now that we have shown (9.2) is a convex optimization problem, we know that pretty much any numerical algorithm will guarantee convergence to the globally optimal solution. 
 
There are many numerical algorithms that can compute optimal solutions of an optimization problem (Nocedal and Wright 1999). Python provides a nice interface, `scipy.optimize`, to many such algorithms, and let us use `scipy.optimize` to solve (9.2) so we can numerically prove our claim. <hr class="ls0"></span><span class="s0">#%% md 
</span>
<span class="s1">**g**. We have provided most of the code necessary for solving (9.2) below. Please fill in the definition of the function `perimeter(u)`, and then run the code. Show your results for $N = 3, 10, 100$. Do the solutions obtained from Python verify our claim? <hr class="ls0"></span><span class="s0">#%% 
</span><span class="s2">import </span><span class="s1">numpy </span><span class="s2">as </span><span class="s1">np</span>
<span class="s2">from </span><span class="s1">scipy</span><span class="s3">.</span><span class="s1">optimize </span><span class="s2">import </span><span class="s1">minimize</span>
<span class="s2">import </span><span class="s1">matplotlib</span><span class="s3">.</span><span class="s1">pyplot </span><span class="s2">as </span><span class="s1">plt</span>

<span class="s0"># -------- Parameters --------</span>
<span class="s1">N </span><span class="s3">= </span><span class="s4">10  </span><span class="s0"># Number of points</span>

<span class="s0"># -------- Objective: polygon perimeter on the unit circle (edge length = 2*sin(u/2)) --------</span>
<span class="s2">def </span><span class="s1">perimeter</span><span class="s3">(</span><span class="s1">u</span><span class="s3">):</span>
    <span class="s0">##############################</span>
    <span class="s0"># TODO BLOCK</span>
    <span class="s0">##############################</span>

<span class="s2">def </span><span class="s1">neg_perimeter</span><span class="s3">(</span><span class="s1">u</span><span class="s3">):</span>
<span class="s0"># SciPy minimizes; negate to perform maximization</span>
    <span class="s2">return </span><span class="s3">-</span><span class="s1">perimeter</span><span class="s3">(</span><span class="s1">u</span><span class="s3">)</span>

<span class="s0"># -------- Constraints &amp; initialization --------</span>
<span class="s0"># Linear equality: sum(a) = 2π</span>
<span class="s1">eq_cons </span><span class="s3">= {</span><span class="s5">'type'</span><span class="s3">: </span><span class="s5">'eq'</span><span class="s3">, </span><span class="s5">'fun'</span><span class="s3">: </span><span class="s2">lambda </span><span class="s1">u</span><span class="s3">: </span><span class="s1">np</span><span class="s3">.</span><span class="s1">sum</span><span class="s3">(</span><span class="s1">u</span><span class="s3">) - </span><span class="s4">2.0 </span><span class="s3">* </span><span class="s1">np</span><span class="s3">.</span><span class="s1">pi</span><span class="s3">}</span>

<span class="s0"># Bounds: u_i ∈ [0, 2π] (upper bound helps numerics)</span>
<span class="s1">bounds </span><span class="s3">= [(</span><span class="s4">0.0</span><span class="s3">, </span><span class="s4">2.0 </span><span class="s3">* </span><span class="s1">np</span><span class="s3">.</span><span class="s1">pi</span><span class="s3">)] * </span><span class="s1">N</span>

<span class="s0"># Initial guess: positive random vector normalized to 2π</span>
<span class="s1">rng </span><span class="s3">= </span><span class="s1">np</span><span class="s3">.</span><span class="s1">random</span><span class="s3">.</span><span class="s1">default_rng</span><span class="s3">(</span><span class="s4">0</span><span class="s3">)</span>
<span class="s1">u0 </span><span class="s3">= </span><span class="s1">rng</span><span class="s3">.</span><span class="s1">random</span><span class="s3">(</span><span class="s1">N</span><span class="s3">)</span>
<span class="s1">u0 </span><span class="s3">= </span><span class="s1">u0 </span><span class="s3">/ </span><span class="s1">u0</span><span class="s3">.</span><span class="s1">sum</span><span class="s3">() * </span><span class="s4">2.0 </span><span class="s3">* </span><span class="s1">np</span><span class="s3">.</span><span class="s1">pi</span>

<span class="s0"># -------- Solve (SLSQP) --------</span>
<span class="s1">res </span><span class="s3">= </span><span class="s1">minimize</span><span class="s3">(</span>
    <span class="s1">neg_perimeter</span><span class="s3">, </span><span class="s1">u0</span><span class="s3">,</span>
    <span class="s1">method</span><span class="s3">=</span><span class="s5">'SLSQP'</span><span class="s3">,</span>
    <span class="s1">bounds</span><span class="s3">=</span><span class="s1">bounds</span><span class="s3">,</span>
    <span class="s1">constraints</span><span class="s3">=[</span><span class="s1">eq_cons</span><span class="s3">],</span>
    <span class="s1">options</span><span class="s3">={</span><span class="s5">'maxiter'</span><span class="s3">: </span><span class="s4">2000</span><span class="s3">, </span><span class="s5">'ftol'</span><span class="s3">: </span><span class="s4">1e-12</span><span class="s3">, </span><span class="s5">'disp'</span><span class="s3">: </span><span class="s2">True</span><span class="s3">}</span>
<span class="s3">)</span>

<span class="s1">uopt </span><span class="s3">= </span><span class="s1">res</span><span class="s3">.</span><span class="s1">x</span>
<span class="s1">print</span><span class="s3">(</span><span class="s5">&quot;Success:&quot;</span><span class="s3">, </span><span class="s1">res</span><span class="s3">.</span><span class="s1">success</span><span class="s3">, </span><span class="s5">&quot;| message:&quot;</span><span class="s3">, </span><span class="s1">res</span><span class="s3">.</span><span class="s1">message</span><span class="s3">)</span>
<span class="s1">print</span><span class="s3">(</span><span class="s5">&quot;Perimeter =&quot;</span><span class="s3">, </span><span class="s1">perimeter</span><span class="s3">(</span><span class="s1">uopt</span><span class="s3">))</span>

<span class="s0"># -------- Recover vertex angles x by cumulative sum (x[0]=0; others accumulate preceding gaps) --------</span>
<span class="s1">x </span><span class="s3">= </span><span class="s1">np</span><span class="s3">.</span><span class="s1">zeros</span><span class="s3">(</span><span class="s1">N</span><span class="s3">)</span>
<span class="s1">x</span><span class="s3">[</span><span class="s4">1</span><span class="s3">:] = </span><span class="s1">np</span><span class="s3">.</span><span class="s1">cumsum</span><span class="s3">(</span><span class="s1">uopt</span><span class="s3">[:-</span><span class="s4">1</span><span class="s3">])</span>

<span class="s0"># -------- Plot --------</span>
<span class="s1">fig</span><span class="s3">, </span><span class="s1">ax </span><span class="s3">= </span><span class="s1">plt</span><span class="s3">.</span><span class="s1">subplots</span><span class="s3">()</span>
<span class="s0"># Draw unit circle</span>
<span class="s1">circle </span><span class="s3">= </span><span class="s1">plt</span><span class="s3">.</span><span class="s1">Circle</span><span class="s3">((</span><span class="s4">0</span><span class="s3">, </span><span class="s4">0</span><span class="s3">), </span><span class="s4">1.0</span><span class="s3">, </span><span class="s1">fill</span><span class="s3">=</span><span class="s2">False</span><span class="s3">)</span>
<span class="s1">ax</span><span class="s3">.</span><span class="s1">add_patch</span><span class="s3">(</span><span class="s1">circle</span><span class="s3">)</span>

<span class="s0"># Scatter vertices</span>
<span class="s1">ax</span><span class="s3">.</span><span class="s1">scatter</span><span class="s3">(</span><span class="s1">np</span><span class="s3">.</span><span class="s1">cos</span><span class="s3">(</span><span class="s1">x</span><span class="s3">), </span><span class="s1">np</span><span class="s3">.</span><span class="s1">sin</span><span class="s3">(</span><span class="s1">x</span><span class="s3">), </span><span class="s1">s</span><span class="s3">=</span><span class="s4">40</span><span class="s3">, </span><span class="s1">label</span><span class="s3">=</span><span class="s5">&quot;points&quot;</span><span class="s3">)</span>

<span class="s1">ax</span><span class="s3">.</span><span class="s1">set_aspect</span><span class="s3">(</span><span class="s5">'equal'</span><span class="s3">, </span><span class="s1">adjustable</span><span class="s3">=</span><span class="s5">'box'</span><span class="s3">)</span>
<span class="s1">ax</span><span class="s3">.</span><span class="s1">set_xlim</span><span class="s3">(-</span><span class="s4">1.1</span><span class="s3">, </span><span class="s4">1.1</span><span class="s3">)</span>
<span class="s1">ax</span><span class="s3">.</span><span class="s1">set_ylim</span><span class="s3">(-</span><span class="s4">1.1</span><span class="s3">, </span><span class="s4">1.1</span><span class="s3">)</span>
<span class="s1">ax</span><span class="s3">.</span><span class="s1">set_xlabel</span><span class="s3">(</span><span class="s5">&quot;x&quot;</span><span class="s3">)</span>
<span class="s1">ax</span><span class="s3">.</span><span class="s1">set_ylabel</span><span class="s3">(</span><span class="s5">&quot;y&quot;</span><span class="s3">)</span>
<span class="s1">ax</span><span class="s3">.</span><span class="s1">set_title</span><span class="s3">(</span><span class="s5">f&quot;N=</span><span class="s2">{</span><span class="s1">N</span><span class="s2">}</span><span class="s5">, perimeter=</span><span class="s2">{</span><span class="s1">perimeter</span><span class="s3">(</span><span class="s1">uopt</span><span class="s3">)</span><span class="s2">:</span><span class="s5">.6f</span><span class="s2">}</span><span class="s5">&quot;</span><span class="s3">)</span>
<span class="s1">ax</span><span class="s3">.</span><span class="s1">legend</span><span class="s3">()</span>
<span class="s1">plt</span><span class="s3">.</span><span class="s1">show</span><span class="s3">()</span><hr class="ls0"><span class="s0">#%% md 
</span><span class="s1">## 2. Convergence proof of Value iteration 
 
Let the Bellman optimality operator be 
$$ 
(T^\star V)(s)=\max_a\Big[\,R(s,a)+\gamma\sum_{s'}P(s'|s,a)V(s')\,\Big], 
\qquad \gamma\in[0,1). 
$$ 
Let $V^\star$ denote the optimal value function, i.e., $V^\star=T^\star V^\star$. 
Value iteration is $V_{k+1}=T^\star V_k$. <hr class="ls0"></span><span class="s0">#%% md 
</span><span class="s1">### 2.1 Contraction 
 
We first prove the operator is a $\gamma$-**contraction**, i.e. 
$$ 
||V_{k+1}-V^\star|| \leq \gamma ||V_k-V^\star|| 
$$ 
 
**(TODO) Answer:** 
 <hr class="ls0"></span><span class="s0">#%% md 
</span><span class="s1">--- 
Answer: 2.1 
The idea is to show that the Bellman optimality operator $T^\star$ brings value functions closer together by at least a factor of $\gamma$. 
 
Value iteration is defined by 
 
$$ 
V_{k+1} = T^\star V_k, 
\qquad V^\star = T^\star V^\star. 
$$ 
 
So the difference at step \(k+1\) is 
 
$$ 
\|V_{k+1} - V^\star\| = \|T^\star V_k - T^\star V^\star\|. 
$$ 
 
For a given \(s\), 
 
$$ 
(T^\star V)(s) = \max_a \Big[ R(s,a) + \gamma \sum_{s'} P(s'|s,a)\, V(s') \Big]. 
$$ 
 
Now compare \(T^\star V_k\) and \(T^\star V^\star\): 
 
$$ 
|(T^\star V_k)(s) - (T^\star V^\star)(s)| 
= \Big|\max_a Q_k(s,a) - \max_a Q^\star(s,a)\Big|, 
$$ 
 
where \(Q_k(s,a)\) and \(Q^\star(s,a)\) are the bracketed terms using \(V_k\) and \(V^\star\). 
 
We know from the triangle inequality theorem that: 
 
$$ 
\big|\max_a x_a - \max_a y_a\big| \le \max_a |x_a - y_a|. 
$$ 
 
So we can bound the difference by 
 
$$ 
|(T^\star V_k)(s) - (T^\star V^\star)(s)| 
\le \max_a \left| \gamma \sum_{s'} P(s'|s,a)\big(V_k(s') - V^\star(s')\big) \right|. 
$$ 
 
$$ 
\le \gamma \max_a \sum_{s'} P(s'|s,a)\, |V_k(s') - V^\star(s')|. 
$$ 
 
Since 
 
$$ 
|V_k(s') - V^\star(s')| \le \|V_k - V^\star\| 
$$ 
 
for every \(s'\), and the probabilities sum to \(1\), we get 
 
$$ 
|(T^\star V_k)(s) - (T^\star V^\star)(s)| \le \gamma \|V_k - V^\star\|. 
$$ 
 
$$ 
\|T^\star V_k - T^\star V^\star\| \le \gamma \|V_k - V^\star\|. 
$$ 
 
This is exactly the contraction property. 
 <hr class="ls0"></span><span class="s0">#%% md 
</span><span class="s1">### 2.2 linear convergence 
Next we prove the convergence is actually **linear**, i.e. 
$$ 
\|V_k-V^\star\|_\infty \leq \gamma^k \|V_0-V^\star\|_\infty 
$$ 
 
**(TODO) Answer:** 
 <hr class="ls0"></span><span class="s0">#%% md 
</span><span class="s1">--- 
**Answer: 2.2** 
 
We show that value iteration converges **linearly** with rate \(\gamma\): 
$$ 
\|V_k - V^\star\|_\infty \le \gamma^k \|V_0 - V^\star\|_\infty. 
$$ 
 
**Proof (via contraction, by induction).** From §2.1 we have the contraction: 
\[ 
\|V_{k+1} - V^\star\|_\infty \le \gamma \,\|V_k - V^\star\|_\infty, \qquad 0\le\gamma&lt;1. 
\] 
 
- **Base case (\(k=0\)).** Trivial: $\|V_0 - V^\star\|_\infty \le \gamma^0 \|V_0 - V^\star\|_\infty$ 
 
- **Inductive step.** Assume $\|V_k - V^\star\|_\infty \le \gamma^k \|V_0 - V^\star\|_\infty$. Then 
$$ 
\|V_{k+1} - V^\star\|_\infty 
\;\le\; \gamma \|V_k - V^\star\|_\infty 
\;\le\; \gamma \cdot \gamma^k \|V_0 - V^\star\|_\infty 
\;=\; \gamma^{k+1} \|V_0 - V^\star\|_\infty. 
$$ 
Thus the claim holds for all $k\ge 0$ 
 
--- <hr class="ls0"></span><span class="s0">#%% md 
</span><span class="s1">### 2.3 Practical stopping rule 
 
In practice we never know what is the true $V^\star$. But what we can calculate is the difference between two iterations. Here we (1) prove an error bound of $\|V-V^\star\|_\infty$ by $\|V_{k+1} - V_k\|_\infty$: 
 
$$ 
\|V_k-V^\star\|_\infty \leq \frac{\|V_{k+1} - V_k\|_\infty}{1-\gamma} 
$$ 
 
and (2) Compute the tolerance on the consecutive-iterate gap $\|V_{k+1}-V_k\|_\infty$ needed to guarantee $\|V - V^\star\|_\infty \le 10^{-6}$ when $\gamma=0.99$. 
 
**(TODO) Answer:** 
 <hr class="ls0"></span><span class="s0">#%% md 
</span><span class="s1">--- 
**Answer: 2.3 (a)** 
 
$$ 
\begin{aligned} 
|V - V^\star\|_\infty 
&amp;= \|V - T^\star V^\star\|_\infty \\ 
&amp;\le \|V - T^\star V\|_\infty \;+\; \|T^\star V - T^\star V^\star\|_\infty \\ 
&amp;\le \|T^\star V - V\|_\infty \;+\; \gamma \|V - V^\star\|_\infty. 
\end{aligned} 
$$ 
 
Rearrange: 
$(1-\gamma)\,\|V - V^\star\|_\infty \;\le\; \|T^\star V - V\|_\infty$ 
which proves the error bound. 
 
Now plug in $V=V_k$. Since $V_{k+1}=T^\star V_k$, 
$$ 
\|V_k - V^\star\|_\infty 
\;\le\; \frac{\|T^\star V_k - V_k\|_\infty}{1-\gamma} 
\;=\; \frac{\|V_{k+1}-V_k\|_\infty}{1-\gamma}. 
$$ 
 
**Answer: 2.3(b)** 
We want $\|V_k - V^\star\|_\infty \le 10^{-6}$ when $\gamma=0.99$. 
Using the bound above, it suffices to ensure 
$$ 
\frac{\|V_{k+1}-V_k\|_\infty}{1-0.99} \;\le\; 10^{-6} 
\;\;\Longleftrightarrow\;\; 
\|V_{k+1}-V_k\|_\infty \;\le\; (1-0.99)\,10^{-6} \;=\; 10^{-8}. 
$$ 
 
--- <hr class="ls0"></span><span class="s0">#%% md 
</span><span class="s1">### 2.4 The bound of iterations 
 
Assume $\|V_1 - V_0\|_\infty = 1$, $\gamma = 0.99$. How much iterations do we need to have $\|V_k - V^\star\|_\infty \leq 10^{-6}$? 
 
**(TODO) Answer:** 
 
 <hr class="ls0"></span><span class="s0">#%% md 
</span><span class="s1">--- 
**Answer: 2.4** 
 
We know from the **linear convergence result** (§2.2): 
 
$$ 
|V_k - V^\star\|_\infty \;\le\; \gamma^k \,\|V_0 - V^\star\|_\infty. 
$$ 
 
But we don’t know $\|V_0 - V^\star\|_\infty$. Instead, we use the **error bound** (§2.3): 
 
$$ 
\|V_0 - V^\star\|_\infty \;\le\; \frac{\|V_1 - V_0\|_\infty}{1-\gamma}. 
$$ 
 
By assumption, $\|V_1 - V_0\|_\infty = 1$ and $\gamma=0.99$ 
So 
$$ 
\|V_0 - V^\star\|_\infty \;\le\; \frac{1}{1-0.99} \;=\; 100. 
$$ 
 
For all \(k\), 
$$ 
\|V_k - V^\star\|_\infty \;\le\; \gamma^k \cdot 100. 
$$ 
 
We want 
$$ 
100 \cdot \gamma^k \;\le\; 10^{-6}. 
$$ 
 
Take logs (with $\gamma=0.99$): 
 
$$ 
\gamma^k \;\le\; 10^{-8} 
\quad\Longrightarrow\quad 
k \;\ge\; \frac{\ln(10^{-8})}{\ln(0.99)}. 
$$ 
 
$$ 
\ln(10^{-8}) = -18.4207, 
\qquad \ln(0.99) \approx -0.0100503 
$$ 
 
So 
$$ 
k \;\ge\; \frac{-18.4207}{-0.0100503} \;\approx\; 1833. 
$$ 
 
With $\|V_1 - V_0\|_\infty=1$ and $\gamma=0.99$, we need about 
 
$$ 
k \;\approx\; 1833 
$$ 
 
iterations to guarantee $\|V_k - V^\star\|_\infty \le 10^{-6}$ 
 
--- <hr class="ls0"></span><span class="s0">#%% md 
</span><span class="s1">## 3. Cliffwalk 
 
Implement policy evaluation, policy improvement, value iteration, and policy iteration for the `CliffWalking` task. For clarity and reproducibility, We include a minimal reimplementation of the environment that mirrors Gymnasium’s dynamics and reward scheme. 
 
 
&lt;figure style=&quot;text-align:center;&quot;&gt; 
  &lt;img src=&quot;https://raw.githubusercontent.com/ComputationalRobotics/2025-ES-AM-158-PSET/main/PSET1/cliffwalk.png&quot; width=&quot;600&quot; alt=&quot;Inscribed polygon&quot;&gt; 
  &lt;figcaption style=&quot;color:#6a737d; font-style:italic;&quot;&gt; 
    Figure 3. Illustration to cliffwalk problem. 
  &lt;/figcaption&gt; 
&lt;/figure&gt; <hr class="ls0"></span><span class="s0">#%% md 
</span><span class="s1">**CliffWalking (Gym-compatible) — Specification** 
 
- **Grid:** 4 rows × 12 columns (row-major indexing; `state_id = row * 12 + col`; index origin at top-left in comments). 
- **Start:** bottom-left cell `(row=3, col=0)`. 
- **Goal:** bottom-right cell `(row=3, col=11)`. 
- **Actions (4):** up (0), right (1), down (2), left (3). 
- **Rewards:** −1 per step; −100 on entering a cliff cell; 0 at the goal. 
- **Termination:** episode ends upon reaching the goal; this states are terminal/absorbing. If reaching cliff will go back to start. 
 
**Transition table** 
 
- `P[state][action] → list[(prob, next_state, reward, done)]` 
- Deterministic dynamics: each list contains a single tuple with `prob = 1.0` after handling boundaries, cliff, and goal. <hr class="ls0"></span><span class="s0">#%% 
</span><span class="s2">import </span><span class="s1">copy</span>
<span class="s2">import </span><span class="s1">numpy </span><span class="s2">as </span><span class="s1">np</span>
<span class="s2">import </span><span class="s1">matplotlib</span><span class="s3">.</span><span class="s1">pyplot </span><span class="s2">as </span><span class="s1">plt</span>

<span class="s2">import </span><span class="s1">gymnasium </span><span class="s2">as </span><span class="s1">gym</span>

<span class="s1">np</span><span class="s3">.</span><span class="s1">set_printoptions</span><span class="s3">(</span><span class="s1">precision</span><span class="s3">=</span><span class="s4">3</span><span class="s3">, </span><span class="s1">suppress</span><span class="s3">=</span><span class="s2">True</span><span class="s3">)</span>

<span class="s0"># Create Gym CliffWalking environment (v1).</span>
<span class="s1">env_gym </span><span class="s3">= </span><span class="s1">gym</span><span class="s3">.</span><span class="s1">make</span><span class="s3">(</span><span class="s5">&quot;CliffWalking-v1&quot;</span><span class="s3">, </span><span class="s1">render_mode</span><span class="s3">=</span><span class="s5">&quot;ansi&quot;</span><span class="s3">)</span>
<span class="s1">nS</span><span class="s3">, </span><span class="s1">nA </span><span class="s3">= </span><span class="s1">env_gym</span><span class="s3">.</span><span class="s1">observation_space</span><span class="s3">.</span><span class="s1">n</span><span class="s3">, </span><span class="s1">env_gym</span><span class="s3">.</span><span class="s1">action_space</span><span class="s3">.</span><span class="s1">n</span>
<span class="s0"># The CliffWalking grid is 4 × 12; actions are 0=UP, 1=RIGHT, 2=DOWN, 3=LEFT.</span>
<span class="s1">print</span><span class="s3">(</span><span class="s5">f&quot;State count=</span><span class="s2">{</span><span class="s1">nS</span><span class="s2">}</span><span class="s5">, Action count=</span><span class="s2">{</span><span class="s1">nA</span><span class="s2">}</span><span class="s5">&quot;</span><span class="s3">)</span>

<span class="s0"># -------------------------------------------------------------------</span>
<span class="s0"># Utility: pretty-print a value function as a 2D grid (nrow × ncol).</span>
<span class="s0"># Values can be any (nS,) array-like; states are indexed row-major:</span>
<span class="s0"># State_id = row * ncol + col</span>
<span class="s0"># -------------------------------------------------------------------</span>
<span class="s2">def </span><span class="s1">print_values</span><span class="s3">(</span><span class="s1">values</span><span class="s3">, </span><span class="s1">nrow</span><span class="s3">: </span><span class="s1">int</span><span class="s3">, </span><span class="s1">ncol</span><span class="s3">: </span><span class="s1">int</span><span class="s3">, </span><span class="s1">title</span><span class="s3">: </span><span class="s1">str </span><span class="s3">= </span><span class="s5">&quot;State Values&quot;</span><span class="s3">):</span>
    <span class="s6">&quot;&quot;&quot;Print a value table in grid form.&quot;&quot;&quot;</span>
    <span class="s1">values </span><span class="s3">= </span><span class="s1">np</span><span class="s3">.</span><span class="s1">asarray</span><span class="s3">(</span><span class="s1">values</span><span class="s3">).</span><span class="s1">reshape</span><span class="s3">(</span><span class="s1">nrow</span><span class="s3">, </span><span class="s1">ncol</span><span class="s3">)</span>
    <span class="s1">print</span><span class="s3">(</span><span class="s1">title</span><span class="s3">)</span>
    <span class="s2">for </span><span class="s1">r </span><span class="s2">in </span><span class="s1">range</span><span class="s3">(</span><span class="s1">nrow</span><span class="s3">):</span>
        <span class="s1">print</span><span class="s3">(</span><span class="s5">&quot; &quot;</span><span class="s3">.</span><span class="s1">join</span><span class="s3">(</span><span class="s5">f&quot;</span><span class="s2">{</span><span class="s1">values</span><span class="s3">[</span><span class="s1">r</span><span class="s3">, </span><span class="s1">c</span><span class="s3">]</span><span class="s2">:</span><span class="s5">6.2f</span><span class="s2">}</span><span class="s5">&quot; </span><span class="s2">for </span><span class="s1">c </span><span class="s2">in </span><span class="s1">range</span><span class="s3">(</span><span class="s1">ncol</span><span class="s3">)))</span>
    <span class="s1">print</span><span class="s3">()</span>

<span class="s0"># -------------------------------------------------------------------</span>
<span class="s0"># Utility: pretty-print a policy on the CliffWalking grid.</span>
<span class="s0"># </span>
<span class="s0"># Accepted pi formats for each state s:</span>
<span class="s0"># - Int a               : deterministic action</span>
<span class="s0"># - Length-4 vector     : Q-values or preferences; we render argmax (ties shown)</span>
<span class="s0"># - Length-4 probabilities (stochastic policy): greedy action(s) by max prob</span>
<span class="s0"># </span>
<span class="s0"># Notes:</span>
<span class="s0"># - Uses Gym's action order: 0=UP(↑), 1=RIGHT(→), 2=DOWN(↓), 3=LEFT(←)</span>
<span class="s0"># - Terminal states in CliffWalking (bottom row except col=0) are marked:</span>
<span class="s0"># S at (last_row, 0), C for cliff cells (last_row, 1..ncol-2), G at (last_row, ncol-1)</span>
<span class="s0"># -------------------------------------------------------------------</span>
<span class="s2">def </span><span class="s1">print_policy</span><span class="s3">(</span><span class="s1">pi</span><span class="s3">, </span><span class="s1">nrow</span><span class="s3">: </span><span class="s1">int</span><span class="s3">, </span><span class="s1">ncol</span><span class="s3">: </span><span class="s1">int</span><span class="s3">, </span><span class="s1">title</span><span class="s3">: </span><span class="s1">str </span><span class="s3">= </span><span class="s5">&quot;Policy&quot;</span><span class="s3">):</span>
    <span class="s6">&quot;&quot;&quot;Print a deterministic/stochastic policy. 
    - If pi is a list of lists (length 4): treat as stochastic over [up, down, left, right]. 
    - We render the greedy direction; if ties exist, we list all best arrows. 
    &quot;&quot;&quot;</span>
    <span class="s1">arrow </span><span class="s3">= {</span><span class="s4">0</span><span class="s3">:</span><span class="s5">&quot;^&quot;</span><span class="s3">, </span><span class="s4">1</span><span class="s3">:</span><span class="s5">&quot;&gt;&quot;</span><span class="s3">, </span><span class="s4">2</span><span class="s3">:</span><span class="s5">&quot;v&quot;</span><span class="s3">, </span><span class="s4">3</span><span class="s3">:</span><span class="s5">&quot;&lt;&quot;</span><span class="s3">}  </span><span class="s0"># Order aligned with env actions in this notebook</span>
    <span class="s1">print</span><span class="s3">(</span><span class="s1">title</span><span class="s3">)</span>
    <span class="s2">for </span><span class="s1">i </span><span class="s2">in </span><span class="s1">range</span><span class="s3">(</span><span class="s1">nrow</span><span class="s3">):</span>
        <span class="s1">row_syms </span><span class="s3">= []</span>
        <span class="s2">for </span><span class="s1">j </span><span class="s2">in </span><span class="s1">range</span><span class="s3">(</span><span class="s1">ncol</span><span class="s3">):</span>
            <span class="s1">s </span><span class="s3">= </span><span class="s1">i</span><span class="s3">*</span><span class="s1">ncol </span><span class="s3">+ </span><span class="s1">j</span>
            <span class="s1">p </span><span class="s3">= </span><span class="s1">pi</span><span class="s3">[</span><span class="s1">s</span><span class="s3">]</span>
            <span class="s0"># Determine best action(s)</span>
            <span class="s2">if </span><span class="s1">isinstance</span><span class="s3">(</span><span class="s1">p</span><span class="s3">, </span><span class="s1">list</span><span class="s3">) </span><span class="s2">and </span><span class="s1">len</span><span class="s3">(</span><span class="s1">p</span><span class="s3">) == </span><span class="s4">4</span><span class="s3">:</span>
                <span class="s1">best </span><span class="s3">= </span><span class="s1">np</span><span class="s3">.</span><span class="s1">argwhere</span><span class="s3">(</span><span class="s1">np</span><span class="s3">.</span><span class="s1">array</span><span class="s3">(</span><span class="s1">p</span><span class="s3">) == </span><span class="s1">np</span><span class="s3">.</span><span class="s1">max</span><span class="s3">(</span><span class="s1">p</span><span class="s3">)).</span><span class="s1">flatten</span><span class="s3">().</span><span class="s1">tolist</span><span class="s3">()</span>
            <span class="s2">elif </span><span class="s1">isinstance</span><span class="s3">(</span><span class="s1">p</span><span class="s3">, </span><span class="s1">int</span><span class="s3">):</span>
                <span class="s1">best </span><span class="s3">= [</span><span class="s1">p</span><span class="s3">]</span>
            <span class="s2">else</span><span class="s3">:</span>
                <span class="s0"># Fallback: greedy over provided vector/array</span>
                <span class="s1">arr </span><span class="s3">= </span><span class="s1">np</span><span class="s3">.</span><span class="s1">array</span><span class="s3">(</span><span class="s1">p</span><span class="s3">, </span><span class="s1">dtype</span><span class="s3">=</span><span class="s1">float</span><span class="s3">).</span><span class="s1">ravel</span><span class="s3">()</span>
                <span class="s1">best </span><span class="s3">= </span><span class="s1">np</span><span class="s3">.</span><span class="s1">argwhere</span><span class="s3">(</span><span class="s1">arr </span><span class="s3">== </span><span class="s1">np</span><span class="s3">.</span><span class="s1">max</span><span class="s3">(</span><span class="s1">arr</span><span class="s3">)).</span><span class="s1">flatten</span><span class="s3">().</span><span class="s1">tolist</span><span class="s3">()</span>
            <span class="s0"># Special case: terminals on bottom row except j==0</span>
            <span class="s2">if </span><span class="s1">i </span><span class="s3">== </span><span class="s1">nrow</span><span class="s3">-</span><span class="s4">1 </span><span class="s2">and </span><span class="s1">j </span><span class="s3">&gt; </span><span class="s4">0</span><span class="s3">:</span>
                <span class="s1">row_syms</span><span class="s3">.</span><span class="s1">append</span><span class="s3">(</span><span class="s5">&quot;T&quot;</span><span class="s3">)</span>
            <span class="s2">else</span><span class="s3">:</span>
                <span class="s1">row_syms</span><span class="s3">.</span><span class="s1">append</span><span class="s3">(</span><span class="s5">&quot;&quot;</span><span class="s3">.</span><span class="s1">join</span><span class="s3">(</span><span class="s1">arrow</span><span class="s3">[</span><span class="s1">a</span><span class="s3">] </span><span class="s2">for </span><span class="s1">a </span><span class="s2">in </span><span class="s1">best</span><span class="s3">))</span>
        <span class="s1">print</span><span class="s3">(</span><span class="s5">&quot; &quot;</span><span class="s3">.</span><span class="s1">join</span><span class="s3">(</span><span class="s1">sym </span><span class="s2">if </span><span class="s1">sym </span><span class="s2">else </span><span class="s5">&quot;.&quot; </span><span class="s2">for </span><span class="s1">sym </span><span class="s2">in </span><span class="s1">row_syms</span><span class="s3">))</span>
    <span class="s1">print</span><span class="s3">(</span><span class="s1">p</span><span class="s3">)</span>
    <span class="s1">print</span><span class="s3">()</span><hr class="ls0"><span class="s0">#%% md 
</span><span class="s1">### 3.1 Define Environment Model (no need to fill in) <hr class="ls0"></span><span class="s0">#%% 
</span><span class="s2">class </span><span class="s1">CliffWalkingEnv</span><span class="s3">:</span>
    <span class="s6">&quot;&quot;&quot;Cliff Walking environment (Gym-compatible dynamics) 
 
    State indexing 
    -------------- 
    - Flattened row-major: state_id = row * ncol + col 
    - Rows: 0..nrow-1 (top → bottom), Cols: 0..ncol-1 (left → right) 
 
    Actions (match Gym/toy_text) 
    ---------------------------- 
    - 0: UP (↑), 1: RIGHT (→), 2: DOWN (↓), 3: LEFT (←) 
 
    Grid (nrow=4, ncol=12) 
    ---------------------- 
        [  0] [  1] [  2] [  3] [  4] [  5] [  6] [  7] [  8] [  9] [ 10] [ 11] 
        [ 12] [ 13] [ 14] [ 15] [ 16] [ 17] [ 18] [ 19] [ 20] [ 21] [ 22] [ 23] 
        [ 24] [ 25] [ 26] [ 27] [ 28] [ 29] [ 30] [ 31] [ 32] [ 33] [ 34] [ 35] 
        [36=S] [37=C] [38=C] [39=C] [40=C] [41=C] [42=C] [43=C] [44=C] [45=C] [46=C] [47=G] 
 
    Legend 
    ------ 
    - S (start):  (row=3, col=0)   -&gt; state 36 
    - C (cliff):  (row=3, col=1..10) -&gt; states 37..46 
    - G (goal):   (row=3, col=11)  -&gt; state 47 
 
    Termination &amp; rewards 
    --------------------- 
    - Stepping into a cliff cell: reward = -100, done = False, go back to start 
    - Any other move:             reward = -1,   done = False 
    - Terminal states are absorbing: once in {goal}, any action keeps you there with reward 0. 
    &quot;&quot;&quot;</span>

    <span class="s0"># Action constants for clarity</span>
    <span class="s1">A_UP</span><span class="s3">, </span><span class="s1">A_RIGHT</span><span class="s3">, </span><span class="s1">A_DOWN</span><span class="s3">, </span><span class="s1">A_LEFT </span><span class="s3">= </span><span class="s4">0</span><span class="s3">, </span><span class="s4">1</span><span class="s3">, </span><span class="s4">2</span><span class="s3">, </span><span class="s4">3</span>

    <span class="s2">def </span><span class="s1">__init__</span><span class="s3">(</span><span class="s1">self</span><span class="s3">, </span><span class="s1">ncol</span><span class="s3">: </span><span class="s1">int </span><span class="s3">= </span><span class="s4">12</span><span class="s3">, </span><span class="s1">nrow</span><span class="s3">: </span><span class="s1">int </span><span class="s3">= </span><span class="s4">4</span><span class="s3">):</span>
        <span class="s1">self</span><span class="s3">.</span><span class="s1">ncol </span><span class="s3">= </span><span class="s1">int</span><span class="s3">(</span><span class="s1">ncol</span><span class="s3">)</span>
        <span class="s1">self</span><span class="s3">.</span><span class="s1">nrow </span><span class="s3">= </span><span class="s1">int</span><span class="s3">(</span><span class="s1">nrow</span><span class="s3">)</span>
        <span class="s1">self</span><span class="s3">.</span><span class="s1">nS </span><span class="s3">= </span><span class="s1">self</span><span class="s3">.</span><span class="s1">nrow </span><span class="s3">* </span><span class="s1">self</span><span class="s3">.</span><span class="s1">ncol</span>
        <span class="s1">self</span><span class="s3">.</span><span class="s1">nA </span><span class="s3">= </span><span class="s4">4</span>
        <span class="s0"># Transition table: P[state][action] = [(prob, next_state, reward, done)]</span>
        <span class="s1">self</span><span class="s3">.</span><span class="s1">P </span><span class="s3">= </span><span class="s1">self</span><span class="s3">.</span><span class="s1">_create_P</span><span class="s3">()</span>

    <span class="s2">def </span><span class="s1">_create_P</span><span class="s3">(</span><span class="s1">self</span><span class="s3">):</span>
    <span class="s0"># Allocate empty transition table</span>
        <span class="s1">P </span><span class="s3">= [[[] </span><span class="s2">for </span><span class="s1">_ </span><span class="s2">in </span><span class="s1">range</span><span class="s3">(</span><span class="s1">self</span><span class="s3">.</span><span class="s1">nA</span><span class="s3">)] </span><span class="s2">for </span><span class="s1">_ </span><span class="s2">in </span><span class="s1">range</span><span class="s3">(</span><span class="s1">self</span><span class="s3">.</span><span class="s1">nS</span><span class="s3">)]</span>

        <span class="s0"># Movement deltas in (dx, dy), matching action order: 0↑, 1→, 2↓, 3←</span>
        <span class="s0"># NOTE: x increases to the right (columns), y increases downward (rows).</span>
        <span class="s1">deltas </span><span class="s3">= {</span>
            <span class="s1">self</span><span class="s3">.</span><span class="s1">A_UP</span><span class="s3">:    ( </span><span class="s4">0</span><span class="s3">, -</span><span class="s4">1</span><span class="s3">),</span>
            <span class="s1">self</span><span class="s3">.</span><span class="s1">A_RIGHT</span><span class="s3">: ( </span><span class="s4">1</span><span class="s3">,  </span><span class="s4">0</span><span class="s3">),  </span><span class="s0"># (1, 0) Written to hint order; same as (1, 0)</span>
            <span class="s1">self</span><span class="s3">.</span><span class="s1">A_DOWN</span><span class="s3">:  ( </span><span class="s4">0</span><span class="s3">,  </span><span class="s4">1</span><span class="s3">),</span>
            <span class="s1">self</span><span class="s3">.</span><span class="s1">A_LEFT</span><span class="s3">:  (-</span><span class="s4">1</span><span class="s3">,  </span><span class="s4">0</span><span class="s3">),</span>
        <span class="s3">}</span>

        <span class="s1">start_s </span><span class="s3">= (</span><span class="s1">self</span><span class="s3">.</span><span class="s1">nrow </span><span class="s3">- </span><span class="s4">1</span><span class="s3">) * </span><span class="s1">self</span><span class="s3">.</span><span class="s1">ncol </span><span class="s3">+ </span><span class="s4">0</span>
        <span class="s1">goal_s  </span><span class="s3">= (</span><span class="s1">self</span><span class="s3">.</span><span class="s1">nrow </span><span class="s3">- </span><span class="s4">1</span><span class="s3">) * </span><span class="s1">self</span><span class="s3">.</span><span class="s1">ncol </span><span class="s3">+ (</span><span class="s1">self</span><span class="s3">.</span><span class="s1">ncol </span><span class="s3">- </span><span class="s4">1</span><span class="s3">)</span>

        <span class="s2">for </span><span class="s1">r </span><span class="s2">in </span><span class="s1">range</span><span class="s3">(</span><span class="s1">self</span><span class="s3">.</span><span class="s1">nrow</span><span class="s3">):</span>
            <span class="s2">for </span><span class="s1">c </span><span class="s2">in </span><span class="s1">range</span><span class="s3">(</span><span class="s1">self</span><span class="s3">.</span><span class="s1">ncol</span><span class="s3">):</span>
                <span class="s1">s </span><span class="s3">= </span><span class="s1">r </span><span class="s3">* </span><span class="s1">self</span><span class="s3">.</span><span class="s1">ncol </span><span class="s3">+ </span><span class="s1">c</span>

                <span class="s2">if </span><span class="s1">r </span><span class="s3">== </span><span class="s1">self</span><span class="s3">.</span><span class="s1">nrow </span><span class="s3">- </span><span class="s4">1 </span><span class="s2">and </span><span class="s1">c </span><span class="s3">&gt; </span><span class="s4">0</span><span class="s3">:</span>
                    <span class="s2">for </span><span class="s1">a </span><span class="s2">in </span><span class="s1">range</span><span class="s3">(</span><span class="s1">self</span><span class="s3">.</span><span class="s1">nA</span><span class="s3">):</span>
                        <span class="s1">P</span><span class="s3">[</span><span class="s1">s</span><span class="s3">][</span><span class="s1">a</span><span class="s3">] = [(</span><span class="s4">1.0</span><span class="s3">, </span><span class="s1">s</span><span class="s3">, </span><span class="s4">0.0</span><span class="s3">, </span><span class="s2">True</span><span class="s3">)]</span>
                    <span class="s2">continue</span>

                <span class="s2">for </span><span class="s1">a </span><span class="s2">in </span><span class="s1">range</span><span class="s3">(</span><span class="s1">self</span><span class="s3">.</span><span class="s1">nA</span><span class="s3">):</span>
                    <span class="s1">dx</span><span class="s3">, </span><span class="s1">dy </span><span class="s3">= </span><span class="s1">deltas</span><span class="s3">[</span><span class="s1">a</span><span class="s3">]</span>

                    <span class="s1">nc </span><span class="s3">= </span><span class="s1">min</span><span class="s3">(</span><span class="s1">self</span><span class="s3">.</span><span class="s1">ncol </span><span class="s3">- </span><span class="s4">1</span><span class="s3">, </span><span class="s1">max</span><span class="s3">(</span><span class="s4">0</span><span class="s3">, </span><span class="s1">c </span><span class="s3">+ </span><span class="s1">dx</span><span class="s3">))</span>
                    <span class="s1">nr </span><span class="s3">= </span><span class="s1">min</span><span class="s3">(</span><span class="s1">self</span><span class="s3">.</span><span class="s1">nrow </span><span class="s3">- </span><span class="s4">1</span><span class="s3">, </span><span class="s1">max</span><span class="s3">(</span><span class="s4">0</span><span class="s3">, </span><span class="s1">r </span><span class="s3">+ </span><span class="s1">dy</span><span class="s3">))</span>

                    <span class="s1">ns </span><span class="s3">= </span><span class="s1">nr </span><span class="s3">* </span><span class="s1">self</span><span class="s3">.</span><span class="s1">ncol </span><span class="s3">+ </span><span class="s1">nc</span>
                    <span class="s1">reward </span><span class="s3">= -</span><span class="s4">1.0</span>
                    <span class="s1">done </span><span class="s3">= </span><span class="s2">False</span>

                    <span class="s2">if </span><span class="s1">nr </span><span class="s3">== </span><span class="s1">self</span><span class="s3">.</span><span class="s1">nrow </span><span class="s3">- </span><span class="s4">1 </span><span class="s2">and </span><span class="s4">1 </span><span class="s3">&lt;= </span><span class="s1">nc </span><span class="s3">&lt;= </span><span class="s1">self</span><span class="s3">.</span><span class="s1">ncol </span><span class="s3">- </span><span class="s4">2</span><span class="s3">:</span>
                        <span class="s1">ns </span><span class="s3">= </span><span class="s1">start_s          </span>
                        <span class="s1">reward </span><span class="s3">= -</span><span class="s4">100.0</span>
                        <span class="s1">done </span><span class="s3">= </span><span class="s2">False</span>

                    <span class="s2">elif </span><span class="s1">nr </span><span class="s3">== </span><span class="s1">self</span><span class="s3">.</span><span class="s1">nrow </span><span class="s3">- </span><span class="s4">1 </span><span class="s2">and </span><span class="s1">nc </span><span class="s3">== </span><span class="s1">self</span><span class="s3">.</span><span class="s1">ncol </span><span class="s3">- </span><span class="s4">1</span><span class="s3">:</span>
                        <span class="s1">done </span><span class="s3">= </span><span class="s2">True</span>

                    <span class="s1">P</span><span class="s3">[</span><span class="s1">s</span><span class="s3">][</span><span class="s1">a</span><span class="s3">] = [(</span><span class="s4">1.0</span><span class="s3">, </span><span class="s1">ns</span><span class="s3">, </span><span class="s1">reward</span><span class="s3">, </span><span class="s1">done</span><span class="s3">)]</span>

        <span class="s2">return </span><span class="s1">P</span>


<span class="s0"># Build environment</span>
<span class="s1">env </span><span class="s3">= </span><span class="s1">CliffWalkingEnv</span><span class="s3">(</span><span class="s1">ncol</span><span class="s3">=</span><span class="s4">12</span><span class="s3">, </span><span class="s1">nrow</span><span class="s3">=</span><span class="s4">4</span><span class="s3">)</span><hr class="ls0"><span class="s0">#%% md 
</span><span class="s1">### 3.2 Policy Evaluation <hr class="ls0"></span><span class="s0">#%% 
</span><span class="s2">def </span><span class="s1">policy_evaluation</span><span class="s3">(</span><span class="s1">env</span><span class="s3">, </span><span class="s1">pi</span><span class="s3">, </span><span class="s1">gamma</span><span class="s3">=</span><span class="s4">0.95</span><span class="s3">, </span><span class="s1">theta</span><span class="s3">=</span><span class="s4">1e-10</span><span class="s3">):</span>
    <span class="s6">&quot;&quot;&quot;Iterative policy evaluation for a given stochastic policy π(a|s). 
 
    Args: 
        env: environment with a tabular transition model env.P where 
             P[s][a] = [(prob, next_state, reward, done)]. 
        pi:  policy probabilities shaped [nS][4]; each pi[s] is a length-4 list 
             over actions [UP, RIGHT, DOWN, LEFT]. 
        gamma: discount factor ∈ [0, 1). 
        theta: convergence threshold on the ∞-norm of value updates. 
 
    Returns: 
        v: list of state values of length nS. 
    &quot;&quot;&quot;</span>
    <span class="s1">nS </span><span class="s3">= </span><span class="s1">env</span><span class="s3">.</span><span class="s1">nrow </span><span class="s3">* </span><span class="s1">env</span><span class="s3">.</span><span class="s1">ncol</span>
    <span class="s1">v </span><span class="s3">= [</span><span class="s4">0.0</span><span class="s3">] * </span><span class="s1">nS  </span><span class="s0"># Initialize V(s)=0</span>
    <span class="s1">it </span><span class="s3">= </span><span class="s4">1  </span><span class="s0"># Iteration counter (logging only)</span>

    <span class="s2">while True</span><span class="s3">:</span>
        <span class="s1">max_diff </span><span class="s3">= </span><span class="s4">0.0</span>
        <span class="s1">new_v </span><span class="s3">= [</span><span class="s4">0.0</span><span class="s3">] * </span><span class="s1">nS</span>

        <span class="s2">for </span><span class="s1">s </span><span class="s2">in </span><span class="s1">range</span><span class="s3">(</span><span class="s1">nS</span><span class="s3">):</span>
            <span class="s1">v_sum </span><span class="s3">= </span><span class="s4">0.0  </span><span class="s0"># Σ_a π(a|s) * Q(s,a)</span>
            <span class="s2">for </span><span class="s1">a </span><span class="s2">in </span><span class="s1">range</span><span class="s3">(</span><span class="s4">4</span><span class="s3">):</span>
                <span class="s1">pi_sa </span><span class="s3">= </span><span class="s1">pi</span><span class="s3">[</span><span class="s1">s</span><span class="s3">][</span><span class="s1">a</span><span class="s3">]</span>
                <span class="s2">for </span><span class="s3">(</span><span class="s1">prob</span><span class="s3">, </span><span class="s1">next_state</span><span class="s3">, </span><span class="s1">reward</span><span class="s3">, </span><span class="s1">done</span><span class="s3">) </span><span class="s2">in </span><span class="s1">env</span><span class="s3">.</span><span class="s1">P</span><span class="s3">[</span><span class="s1">s</span><span class="s3">][</span><span class="s1">a</span><span class="s3">]:</span>
                    <span class="s1">v_sum </span><span class="s3">+= </span><span class="s1">pi_sa </span><span class="s3">* </span><span class="s1">prob </span><span class="s3">* (</span><span class="s1">reward </span><span class="s3">+ (</span><span class="s4">0.0 </span><span class="s2">if </span><span class="s1">done </span><span class="s2">else </span><span class="s1">gamma </span><span class="s3">* </span><span class="s1">v</span><span class="s3">[</span><span class="s1">next_state</span><span class="s3">]))</span>

            <span class="s1">new_v</span><span class="s3">[</span><span class="s1">s</span><span class="s3">] = </span><span class="s1">v_sum</span>
            <span class="s1">max_diff </span><span class="s3">= </span><span class="s1">max</span><span class="s3">(</span><span class="s1">max_diff</span><span class="s3">, </span><span class="s1">abs</span><span class="s3">(</span><span class="s1">new_v</span><span class="s3">[</span><span class="s1">s</span><span class="s3">] - </span><span class="s1">v</span><span class="s3">[</span><span class="s1">s</span><span class="s3">]))</span>

        <span class="s1">v </span><span class="s3">= </span><span class="s1">new_v</span>
        <span class="s2">if </span><span class="s1">max_diff </span><span class="s3">&lt; </span><span class="s1">theta</span><span class="s3">:</span>
            <span class="s2">break</span>
        <span class="s1">it </span><span class="s3">+= </span><span class="s4">1</span>

    <span class="s1">print</span><span class="s3">(</span><span class="s5">f&quot;Policy evaluation converged in </span><span class="s2">{</span><span class="s1">it</span><span class="s2">} </span><span class="s5">iteration(s).&quot;</span><span class="s3">)</span>
    <span class="s2">return </span><span class="s1">v</span>


<span class="s0"># --- Example: evaluate a uniform random policy ---</span>
<span class="s1">pi </span><span class="s3">= [[</span><span class="s4">0.25</span><span class="s3">, </span><span class="s4">0.25</span><span class="s3">, </span><span class="s4">0.25</span><span class="s3">, </span><span class="s4">0.25</span><span class="s3">] </span><span class="s2">for </span><span class="s1">_ </span><span class="s2">in </span><span class="s1">range</span><span class="s3">(</span><span class="s1">env</span><span class="s3">.</span><span class="s1">nrow </span><span class="s3">* </span><span class="s1">env</span><span class="s3">.</span><span class="s1">ncol</span><span class="s3">)]</span>
<span class="s1">gamma </span><span class="s3">= </span><span class="s4">0.95</span>

<span class="s1">v </span><span class="s3">= </span><span class="s1">policy_evaluation</span><span class="s3">(</span><span class="s1">env</span><span class="s3">, </span><span class="s1">pi</span><span class="s3">, </span><span class="s1">gamma</span><span class="s3">)</span>

<span class="s0"># Pretty-print the value function as a 4×12 grid</span>
<span class="s1">print_values</span><span class="s3">(</span><span class="s1">v</span><span class="s3">, </span><span class="s1">env</span><span class="s3">.</span><span class="s1">nrow</span><span class="s3">, </span><span class="s1">env</span><span class="s3">.</span><span class="s1">ncol</span><span class="s3">, </span><span class="s1">title</span><span class="s3">=</span><span class="s5">&quot;Value Function under Random Policy&quot;</span><span class="s3">)</span><hr class="ls0"><span class="s0">#%% md 
</span><span class="s1">### 3.3 Policy Iteration 
 
Policy Iteration alternates between: 
1) **Policy Evaluation**: compute the state-value function $V^{\pi}$ of the current policy $\pi$ 
2) **Policy Improvement**: update $\pi$ to be greedy w.r.t. $V^{\pi}$ <hr class="ls0"></span><span class="s0">#%% 
</span><span class="s2">def </span><span class="s1">policy_improvement</span><span class="s3">(</span><span class="s1">env</span><span class="s3">, </span><span class="s1">pi</span><span class="s3">, </span><span class="s1">v</span><span class="s3">, </span><span class="s1">gamma</span><span class="s3">=</span><span class="s4">0.95</span><span class="s3">):</span>
    <span class="s6">&quot;&quot;&quot;Greedy policy improvement w.r.t. the current state-value function V. 
 
    For each state s: 
      1) Compute Q(s,a) = Σ_{s'} P(s'|s,a)[ r(s,a,s') + γ V(s') ] for all a. 
      2) Find the action(s) with maximal Q(s,a). 
      3) Update π(·|s) to split probability uniformly among all maximizers (tie-aware). 
 
    Args: 
        env: Tabular environment with transitions env.P where 
             P[s][a] = [(prob, next_state, reward, done)]. 
        pi:  Current (possibly stochastic) policy, shape [nS][4]; updated in-place. 
        v:   Current state-value function V(s), length nS. 
        gamma: Discount factor. 
 
    Returns: 
        pi: The improved policy (same object, updated in-place). 
    &quot;&quot;&quot;</span>
    <span class="s1">nS </span><span class="s3">= </span><span class="s1">env</span><span class="s3">.</span><span class="s1">nrow </span><span class="s3">* </span><span class="s1">env</span><span class="s3">.</span><span class="s1">ncol</span>
    <span class="s1">nA </span><span class="s3">= </span><span class="s4">4</span>
    <span class="s1">eps </span><span class="s3">= </span><span class="s4">1e-8  </span><span class="s0"># Numerical tolerance for tie-breaking</span>

    <span class="s2">for </span><span class="s1">s </span><span class="s2">in </span><span class="s1">range</span><span class="s3">(</span><span class="s1">nS</span><span class="s3">):</span>
        <span class="s1">q_list </span><span class="s3">= []</span>
        <span class="s2">for </span><span class="s1">a </span><span class="s2">in </span><span class="s1">range</span><span class="s3">(</span><span class="s1">nA</span><span class="s3">):</span>
            <span class="s1">q_sa </span><span class="s3">= </span><span class="s4">0.0</span>
            <span class="s2">for </span><span class="s3">(</span><span class="s1">prob</span><span class="s3">, </span><span class="s1">next_state</span><span class="s3">, </span><span class="s1">reward</span><span class="s3">, </span><span class="s1">done</span><span class="s3">) </span><span class="s2">in </span><span class="s1">env</span><span class="s3">.</span><span class="s1">P</span><span class="s3">[</span><span class="s1">s</span><span class="s3">][</span><span class="s1">a</span><span class="s3">]:</span>
                <span class="s1">q_sa </span><span class="s3">+= </span><span class="s1">prob </span><span class="s3">* (</span><span class="s1">reward </span><span class="s3">+ (</span><span class="s4">0.0 </span><span class="s2">if </span><span class="s1">done </span><span class="s2">else </span><span class="s1">gamma </span><span class="s3">* </span><span class="s1">v</span><span class="s3">[</span><span class="s1">next_state</span><span class="s3">]))</span>
            <span class="s1">q_list</span><span class="s3">.</span><span class="s1">append</span><span class="s3">(</span><span class="s1">q_sa</span><span class="s3">)</span>
        <span class="s1">best_a </span><span class="s3">= </span><span class="s1">np</span><span class="s3">.</span><span class="s1">argmax</span><span class="s3">(</span><span class="s1">q_list</span><span class="s3">)</span>

        <span class="s1">pi</span><span class="s3">[</span><span class="s1">s</span><span class="s3">] = [</span><span class="s4">1.0 </span><span class="s2">if </span><span class="s1">a </span><span class="s3">== </span><span class="s1">best_a </span><span class="s2">else </span><span class="s4">0.0 </span><span class="s2">for </span><span class="s1">a </span><span class="s2">in </span><span class="s1">range</span><span class="s3">(</span><span class="s1">nA</span><span class="s3">)]</span>

    <span class="s1">print</span><span class="s3">(</span><span class="s5">&quot;Policy improvement completed.&quot;</span><span class="s3">)</span>
    <span class="s2">return </span><span class="s1">pi</span>


<span class="s0"># --- Policy Iteration loop ---</span>
<span class="s1">pi </span><span class="s3">= [[</span><span class="s4">0.25</span><span class="s3">, </span><span class="s4">0.25</span><span class="s3">, </span><span class="s4">0.25</span><span class="s3">, </span><span class="s4">0.25</span><span class="s3">] </span><span class="s2">for </span><span class="s1">_ </span><span class="s2">in </span><span class="s1">range</span><span class="s3">(</span><span class="s1">env</span><span class="s3">.</span><span class="s1">nrow </span><span class="s3">* </span><span class="s1">env</span><span class="s3">.</span><span class="s1">ncol</span><span class="s3">)]</span>
<span class="s1">iters </span><span class="s3">= </span><span class="s4">0</span>
<span class="s2">while True</span><span class="s3">:</span>
    <span class="s1">v </span><span class="s3">= </span><span class="s1">policy_evaluation</span><span class="s3">(</span><span class="s1">env</span><span class="s3">, </span><span class="s1">pi</span><span class="s3">, </span><span class="s1">gamma</span><span class="s3">)</span>
    <span class="s1">old_pi </span><span class="s3">= </span><span class="s1">copy</span><span class="s3">.</span><span class="s1">deepcopy</span><span class="s3">(</span><span class="s1">pi</span><span class="s3">)</span>
    <span class="s1">new_pi </span><span class="s3">= </span><span class="s1">policy_improvement</span><span class="s3">(</span><span class="s1">env</span><span class="s3">, </span><span class="s1">pi</span><span class="s3">, </span><span class="s1">v</span><span class="s3">, </span><span class="s1">gamma</span><span class="s3">)</span>
    <span class="s1">iters </span><span class="s3">+= </span><span class="s4">1</span>
    <span class="s2">if </span><span class="s1">old_pi </span><span class="s3">== </span><span class="s1">new_pi</span><span class="s3">:  </span><span class="s0"># Policy is stable</span>
        <span class="s1">print</span><span class="s3">(</span><span class="s5">f&quot;Policy iteration converged in </span><span class="s2">{</span><span class="s1">iters</span><span class="s2">} </span><span class="s5">improvement step(s).&quot;</span><span class="s3">)</span>
        <span class="s2">break</span>

<span class="s0"># Report results</span>
<span class="s1">print_values</span><span class="s3">(</span><span class="s1">v</span><span class="s3">, </span><span class="s1">env</span><span class="s3">.</span><span class="s1">nrow</span><span class="s3">, </span><span class="s1">env</span><span class="s3">.</span><span class="s1">ncol</span><span class="s3">, </span><span class="s1">title</span><span class="s3">=</span><span class="s5">&quot;Optimal Value Function&quot;</span><span class="s3">)</span>
<span class="s1">print_policy</span><span class="s3">(</span><span class="s1">pi</span><span class="s3">, </span><span class="s1">env</span><span class="s3">.</span><span class="s1">nrow</span><span class="s3">, </span><span class="s1">env</span><span class="s3">.</span><span class="s1">ncol</span><span class="s3">, </span><span class="s1">title</span><span class="s3">=</span><span class="s5">&quot;Optimal Policy&quot;</span><span class="s3">)</span><hr class="ls0"><span class="s0">#%% md 
</span><span class="s1">### 3.4 Value Iteration 
 
Value Iteration applies **Bellman optimality** updates directly to $V$. Or one can treat value iteration as one step policy evaluation plus one step policy improvement. 
 
After convergence, extract the greedy policy. <hr class="ls0"></span><span class="s0">#%% 
</span><span class="s2">def </span><span class="s1">iterate</span><span class="s3">(</span><span class="s1">env</span><span class="s3">, </span><span class="s1">gamma</span><span class="s3">=</span><span class="s4">0.95</span><span class="s3">, </span><span class="s1">theta</span><span class="s3">=</span><span class="s4">1e-10</span><span class="s3">):</span>
    <span class="s6">&quot;&quot;&quot;Value Iteration. 
 
    Updates V(s) &lt;- max_a Σ_{s'} P(s'|s,a) [ r(s,a,s') + γ V(s') ] 
    until the maximum state-wise change is below `theta`. 
 
    Args: 
        env: Tabular environment exposing env.P with 
             P[s][a] = [(prob, next_state, reward, done)] and grid sizes nrow, ncol. 
        gamma (float): Discount factor in [0, 1). 
        theta (float): Convergence threshold on the infinity-norm of value updates. 
 
    Returns: 
        list[float]: The converged state-value function V of length nS (= nrow * ncol). 
 
    Notes: 
        - Terminal states are modeled as absorbing with reward 0 in `env.P`. 
          The Bellman backup naturally yields V(terminal) = 0. 
        - `deltas` (max per-iteration change) is tracked for debugging but not returned. 
    &quot;&quot;&quot;</span>
    <span class="s1">nS</span><span class="s3">, </span><span class="s1">nA </span><span class="s3">= </span><span class="s1">env</span><span class="s3">.</span><span class="s1">nrow </span><span class="s3">* </span><span class="s1">env</span><span class="s3">.</span><span class="s1">ncol</span><span class="s3">, </span><span class="s4">4</span>
    <span class="s1">deltas </span><span class="s3">= []</span>
    <span class="s1">iters </span><span class="s3">= </span><span class="s4">0</span>
    <span class="s1">v </span><span class="s3">= [</span><span class="s4">0.0</span><span class="s3">] * </span><span class="s1">nS</span>

    <span class="s2">while True</span><span class="s3">:</span>
        <span class="s1">iters </span><span class="s3">+= </span><span class="s4">1</span>
        <span class="s1">max_diff </span><span class="s3">= </span><span class="s4">0.0</span>
        <span class="s1">new_v </span><span class="s3">= [</span><span class="s4">0.0</span><span class="s3">] * </span><span class="s1">nS</span>

        <span class="s2">for </span><span class="s1">s </span><span class="s2">in </span><span class="s1">range</span><span class="s3">(</span><span class="s1">nS</span><span class="s3">):</span>
            <span class="s0"># Bellman optimality backup: V(s) = max_a Q(s,a)</span>
            <span class="s1">q_list </span><span class="s3">= []</span>
            <span class="s2">for </span><span class="s1">a </span><span class="s2">in </span><span class="s1">range</span><span class="s3">(</span><span class="s1">nA</span><span class="s3">):</span>
                <span class="s1">q_sa </span><span class="s3">= </span><span class="s4">0.0</span>
                <span class="s2">for </span><span class="s3">(</span><span class="s1">prob</span><span class="s3">, </span><span class="s1">next_state</span><span class="s3">, </span><span class="s1">reward</span><span class="s3">, </span><span class="s1">done</span><span class="s3">) </span><span class="s2">in </span><span class="s1">env</span><span class="s3">.</span><span class="s1">P</span><span class="s3">[</span><span class="s1">s</span><span class="s3">][</span><span class="s1">a</span><span class="s3">]:</span>
                    <span class="s1">q_sa </span><span class="s3">= </span><span class="s1">prob </span><span class="s3">* (</span><span class="s1">reward </span><span class="s3">+ (</span><span class="s4">0.0 </span><span class="s2">if </span><span class="s1">done </span><span class="s2">else </span><span class="s1">gamma </span><span class="s3">* </span><span class="s1">v</span><span class="s3">[</span><span class="s1">next_state</span><span class="s3">]))</span>
                    <span class="s1">q_list</span><span class="s3">.</span><span class="s1">append</span><span class="s3">(</span><span class="s1">q_sa</span><span class="s3">)</span>

            <span class="s1">new_v</span><span class="s3">[</span><span class="s1">s</span><span class="s3">] = </span><span class="s1">max</span><span class="s3">(</span><span class="s1">q_list</span><span class="s3">)</span>
            <span class="s1">max_diff </span><span class="s3">= </span><span class="s1">max</span><span class="s3">(</span><span class="s1">max_diff</span><span class="s3">, </span><span class="s1">abs</span><span class="s3">(</span><span class="s1">new_v</span><span class="s3">[</span><span class="s1">s</span><span class="s3">] - </span><span class="s1">v</span><span class="s3">[</span><span class="s1">s</span><span class="s3">]))</span>

        <span class="s1">v </span><span class="s3">= </span><span class="s1">new_v</span>
        <span class="s1">deltas</span><span class="s3">.</span><span class="s1">append</span><span class="s3">(</span><span class="s1">max_diff</span><span class="s3">)</span>
        <span class="s2">if </span><span class="s1">max_diff </span><span class="s3">&lt; </span><span class="s1">theta</span><span class="s3">:</span>
            <span class="s2">break</span>
    <span class="s1">print</span><span class="s3">(</span><span class="s1">iters</span><span class="s3">)</span>
    <span class="s2">return </span><span class="s1">v</span>


<span class="s2">def </span><span class="s1">greedy_policy</span><span class="s3">(</span><span class="s1">env</span><span class="s3">, </span><span class="s1">v</span><span class="s3">, </span><span class="s1">gamma</span><span class="s3">=</span><span class="s4">0.95</span><span class="s3">):</span>
    <span class="s6">&quot;&quot;&quot;Extract a greedy (tie-aware) policy from a value function. 
 
    For each state s, compute Q(s,a) and set π(a|s)=1/k for all actions a that 
    achieve the maximal Q-value (ties split uniformly); 0 otherwise. 
 
    Args: 
        env: Tabular environment with env.P. 
        v (list[float]): State-value function V(s). 
        gamma (float): Discount factor. 
 
    Returns: 
        list[list[float]]: Policy π of shape [nS][4], each row summing to 1. 
    &quot;&quot;&quot;</span>
    <span class="s1">nS</span><span class="s3">, </span><span class="s1">nA </span><span class="s3">= </span><span class="s1">env</span><span class="s3">.</span><span class="s1">nrow </span><span class="s3">* </span><span class="s1">env</span><span class="s3">.</span><span class="s1">ncol</span><span class="s3">, </span><span class="s4">4</span>
    <span class="s1">pi </span><span class="s3">= [[</span><span class="s4">0.0</span><span class="s3">] * </span><span class="s1">nA </span><span class="s2">for </span><span class="s1">_ </span><span class="s2">in </span><span class="s1">range</span><span class="s3">(</span><span class="s1">nS</span><span class="s3">)]</span>
    <span class="s1">eps </span><span class="s3">= </span><span class="s4">1e-8  </span><span class="s0"># Numerical tolerance for tie detection</span>

    <span class="s2">for </span><span class="s1">s </span><span class="s2">in </span><span class="s1">range</span><span class="s3">(</span><span class="s1">nS</span><span class="s3">):</span>
        <span class="s1">q_list </span><span class="s3">= []</span>
        <span class="s2">for </span><span class="s1">a </span><span class="s2">in </span><span class="s1">range</span><span class="s3">(</span><span class="s1">nA</span><span class="s3">):</span>
            <span class="s1">q </span><span class="s3">= </span><span class="s4">0.0</span>
            <span class="s2">for </span><span class="s3">(</span><span class="s1">p</span><span class="s3">, </span><span class="s1">next_state</span><span class="s3">, </span><span class="s1">r</span><span class="s3">, </span><span class="s1">done</span><span class="s3">) </span><span class="s2">in </span><span class="s1">env</span><span class="s3">.</span><span class="s1">P</span><span class="s3">[</span><span class="s1">s</span><span class="s3">][</span><span class="s1">a</span><span class="s3">]:</span>
                <span class="s1">q </span><span class="s3">+= </span><span class="s1">p </span><span class="s3">* (</span><span class="s1">r </span><span class="s2">if </span><span class="s1">done </span><span class="s2">else </span><span class="s1">r </span><span class="s3">+ </span><span class="s1">gamma </span><span class="s3">* </span><span class="s1">v</span><span class="s3">[</span><span class="s1">next_state</span><span class="s3">])</span>
            <span class="s1">q_list</span><span class="s3">.</span><span class="s1">append</span><span class="s3">(</span><span class="s1">q</span><span class="s3">)</span>

        <span class="s1">q_list </span><span class="s3">= </span><span class="s1">np</span><span class="s3">.</span><span class="s1">array</span><span class="s3">(</span><span class="s1">q_list</span><span class="s3">, </span><span class="s1">dtype</span><span class="s3">=</span><span class="s1">float</span><span class="s3">)</span>
        <span class="s1">max_q </span><span class="s3">= </span><span class="s1">q_list</span><span class="s3">.</span><span class="s1">max</span><span class="s3">()</span>
        <span class="s0"># Tie-aware argmax</span>
        <span class="s1">opt_u </span><span class="s3">= </span><span class="s1">np</span><span class="s3">.</span><span class="s1">isclose</span><span class="s3">(</span><span class="s1">q_list</span><span class="s3">, </span><span class="s1">max_q</span><span class="s3">, </span><span class="s1">rtol</span><span class="s3">=</span><span class="s4">0.0</span><span class="s3">, </span><span class="s1">atol</span><span class="s3">=</span><span class="s1">eps</span><span class="s3">)</span>
        <span class="s1">k </span><span class="s3">= </span><span class="s1">int</span><span class="s3">(</span><span class="s1">opt_u</span><span class="s3">.</span><span class="s1">sum</span><span class="s3">())</span>
        <span class="s1">pi</span><span class="s3">[</span><span class="s1">s</span><span class="s3">] = (</span><span class="s1">opt_u </span><span class="s3">/ </span><span class="s1">k</span><span class="s3">).</span><span class="s1">astype</span><span class="s3">(</span><span class="s1">float</span><span class="s3">).</span><span class="s1">tolist</span><span class="s3">()</span>

    <span class="s2">return </span><span class="s1">pi</span>


<span class="s0"># ----- Run Value Iteration and extract greedy policy -----</span>
<span class="s1">gamma </span><span class="s3">= </span><span class="s4">0.95  </span><span class="s0"># Discount factor</span>
<span class="s1">v </span><span class="s3">= </span><span class="s1">iterate</span><span class="s3">(</span><span class="s1">env</span><span class="s3">, </span><span class="s1">gamma</span><span class="s3">=</span><span class="s1">gamma</span><span class="s3">)  </span><span class="s0"># Assumes `env` is already constructed</span>
<span class="s1">pi </span><span class="s3">= </span><span class="s1">greedy_policy</span><span class="s3">(</span><span class="s1">env</span><span class="s3">, </span><span class="s1">v</span><span class="s3">, </span><span class="s1">gamma</span><span class="s3">=</span><span class="s1">gamma</span><span class="s3">)</span>

<span class="s0"># Pretty-print results (assumes `print_values` and `print_policy` are defined)</span>
<span class="s1">print_values</span><span class="s3">(</span><span class="s1">v</span><span class="s3">, </span><span class="s1">env</span><span class="s3">.</span><span class="s1">nrow</span><span class="s3">, </span><span class="s1">env</span><span class="s3">.</span><span class="s1">ncol</span><span class="s3">, </span><span class="s1">title</span><span class="s3">=</span><span class="s5">&quot;Optimal Value Function (Value Iteration)&quot;</span><span class="s3">)</span>
<span class="s1">print_policy</span><span class="s3">(</span><span class="s1">pi</span><span class="s3">, </span><span class="s1">env</span><span class="s3">.</span><span class="s1">nrow</span><span class="s3">, </span><span class="s1">env</span><span class="s3">.</span><span class="s1">ncol</span><span class="s3">, </span><span class="s1">title</span><span class="s3">=</span><span class="s5">&quot;Optimal Policy (Value Iteration)&quot;</span><span class="s3">)</span><hr class="ls0"><span class="s0">#%% md 
</span><span class="s1">## 4. Matrix–vector Representation of DP 
 
We’ll use a small finite MDP ($|X|=16$, $|A|=4$) and **matrix** forms to compute the optimal $Q$-function: 
 
4.1. Build the **transition matrix** 
   $ 
   P \in \mathbb{R}^{|X||A|\times |X|} 
   $ 
   and the **immediate reward** vector 
   $ 
   r \in \mathbb{R}^{|X||A|}. 
   $ 
 
4.2. Using the matrix form of the $Q$-value function $Q_\pi$ and the value function $V_\pi$ to write down the bellman equation. 
 
4.3. Define the **Bellman optimality operator**: 
   $$ 
   T^\star(Q) = r + \gamma\, P\, J_Q, 
   $$ 
   where 
   $$ 
   (J_Q)(x) = \max_{a} Q(x,a). 
   $$ 
   Iterating $Q_{k+1} = T^\star(Q_k)$ converges to the optimal $Q^\star$. <hr class="ls0"></span><span class="s0">#%% md 
</span><span class="s1">### 4×4 Gridworld — From Bottom‑Left (Start) to Top‑Right (Goal) 
 
**States:** 16 cells in a 4×4 grid, row-major indexing with top-left as (row=0, col=0). 
State id: `s = row * 4 + col`, rows increase downward. 
 
**Start:** bottom-left `(row=3, col=0)` → `s_start = 12` 
**Goal:** top-right `(row=0, col=3)` → `s_goal = 3` 
 
**Actions (4):** 
- `a=0` → UP (↑) 
- `a=1` → RIGHT (→) 
- `a=2` → DOWN (↓) 
- `a=3` → LEFT (←) 
 
**Dynamics:** Deterministic. If an action would leave the grid world, the agent stays in place. 
 
**Rewards (maximize):** 
- `-1` per step 
- `0` in the goal 
 
**Terminal:** The goal is absorbing (from goal, any action keeps you at goal with reward 0). <hr class="ls0"></span><span class="s0">#%% 
# Grid size</span>
<span class="s1">nrow</span><span class="s3">, </span><span class="s1">ncol </span><span class="s3">= </span><span class="s4">4</span><span class="s3">, </span><span class="s4">4</span>
<span class="s1">nS </span><span class="s3">= </span><span class="s1">nrow </span><span class="s3">* </span><span class="s1">ncol  </span><span class="s0"># |X| = 16</span>
<span class="s1">nA </span><span class="s3">= </span><span class="s4">4  </span><span class="s0"># |A| = 4 (UP, RIGHT, DOWN, LEFT)</span>

<span class="s0"># Start (bottom-left) and Goal (top-right)</span>
<span class="s1">s_start </span><span class="s3">= (</span><span class="s1">nrow </span><span class="s3">- </span><span class="s4">1</span><span class="s3">) * </span><span class="s1">ncol </span><span class="s3">+ </span><span class="s4">0  </span><span class="s0"># 12</span>
<span class="s1">s_goal  </span><span class="s3">= </span><span class="s4">0 </span><span class="s3">* </span><span class="s1">ncol </span><span class="s3">+ (</span><span class="s1">ncol </span><span class="s3">- </span><span class="s4">1</span><span class="s3">)  </span><span class="s0"># 3</span>

<span class="s0"># Row-major state id</span>
<span class="s2">def </span><span class="s1">s_id</span><span class="s3">(</span><span class="s1">r</span><span class="s3">, </span><span class="s1">c</span><span class="s3">):</span>
    <span class="s2">return </span><span class="s1">r </span><span class="s3">* </span><span class="s1">ncol </span><span class="s3">+ </span><span class="s1">c</span>

<span class="s0"># For state-action row index in matrices of shape (nS*nA, ...)</span>
<span class="s2">def </span><span class="s1">sa_id</span><span class="s3">(</span><span class="s1">s</span><span class="s3">, </span><span class="s1">a</span><span class="s3">):</span>
    <span class="s2">return </span><span class="s1">s </span><span class="s3">* </span><span class="s1">nA </span><span class="s3">+ </span><span class="s1">a</span>

<span class="s0"># Action deltas: 0=UP, 1=RIGHT, 2=DOWN, 3=LEFT</span>
<span class="s1">DELTAS </span><span class="s3">= {</span>
    <span class="s4">0</span><span class="s3">: (-</span><span class="s4">1</span><span class="s3">,  </span><span class="s4">0</span><span class="s3">),  </span><span class="s0"># UP:    row-1</span>
    <span class="s4">1</span><span class="s3">: ( </span><span class="s4">0</span><span class="s3">,  </span><span class="s4">1</span><span class="s3">),  </span><span class="s0"># RIGHT: col+1</span>
    <span class="s4">2</span><span class="s3">: ( </span><span class="s4">1</span><span class="s3">,  </span><span class="s4">0</span><span class="s3">),  </span><span class="s0"># DOWN:  row+1</span>
    <span class="s4">3</span><span class="s3">: ( </span><span class="s4">0</span><span class="s3">, -</span><span class="s4">1</span><span class="s3">),  </span><span class="s0"># LEFT:  col-1</span>
<span class="s3">}</span>

<span class="s0"># Quick sanity checks and a tiny ASCII map</span>
<span class="s1">print</span><span class="s3">(</span><span class="s5">&quot;Grid 4×4. Start=S (row=3,col=0), Goal=G (row=0,col=3)&quot;</span><span class="s3">)</span>
<span class="s2">for </span><span class="s1">rrow </span><span class="s2">in </span><span class="s1">range</span><span class="s3">(</span><span class="s1">nrow</span><span class="s3">):</span>
    <span class="s1">line </span><span class="s3">= []</span>
    <span class="s2">for </span><span class="s1">ccol </span><span class="s2">in </span><span class="s1">range</span><span class="s3">(</span><span class="s1">ncol</span><span class="s3">):</span>
        <span class="s1">s </span><span class="s3">= </span><span class="s1">s_id</span><span class="s3">(</span><span class="s1">rrow</span><span class="s3">, </span><span class="s1">ccol</span><span class="s3">)</span>
        <span class="s2">if </span><span class="s1">s </span><span class="s3">== </span><span class="s1">s_start</span><span class="s3">:</span>
            <span class="s1">line</span><span class="s3">.</span><span class="s1">append</span><span class="s3">(</span><span class="s5">&quot; S &quot;</span><span class="s3">)</span>
        <span class="s2">elif </span><span class="s1">s </span><span class="s3">== </span><span class="s1">s_goal</span><span class="s3">:</span>
            <span class="s1">line</span><span class="s3">.</span><span class="s1">append</span><span class="s3">(</span><span class="s5">&quot; G &quot;</span><span class="s3">)</span>
        <span class="s2">else</span><span class="s3">:</span>
            <span class="s1">line</span><span class="s3">.</span><span class="s1">append</span><span class="s3">(</span><span class="s5">f&quot;</span><span class="s2">{</span><span class="s1">s</span><span class="s2">:</span><span class="s5">2d</span><span class="s2">}</span><span class="s5">&quot;</span><span class="s3">)</span>
    <span class="s1">print</span><span class="s3">(</span><span class="s5">&quot; &quot;</span><span class="s3">.</span><span class="s1">join</span><span class="s3">(</span><span class="s1">line</span><span class="s3">))</span>
    <hr class="ls0"><span class="s0">#%% md 
</span><span class="s1">### 4.1 Build Transition Matrix and Reward Vector 
 
**Definition:** 
 
- **Transition matrix** $P \in \mathbb{R}^{|X||A|\times |X|}$ 
  Rows index state–action pairs $(x,a)$, columns index next states $x'$. 
  Entry: 
  $$ 
  P[(x,a),\,x'] \;\equiv\; \Pr\{X_{t+1}=x' \mid X_t=x,\; A_t=a\}. 
  $$ 
  Row-wise normalization holds: $\sum_{x'} P[(x,a),x'] = 1$ for every $(x,a)$. 
 
- **Reward vector** $r \in \mathbb{R}^{|X||A|}$ (reward maximization form) 
  Each entry is the one-step expected reward under $(x,a)$: 
  $$ 
  r[(x,a)] \;\equiv\; \mathbb{E}\big[\,R_{t+1}\mid X_t=x,\; A_t=a\,\big]. 
  $$ 
 
**Indexing note.** 
A convenient index for $(x,a)$ is 
$$ 
i = x\,|A| + a 
$$ <hr class="ls0"></span><span class="s0">#%% 
</span>
<span class="s0"># Build P (|X||A| × |X|) and r (|X||A|)</span>
<span class="s1">P </span><span class="s3">= </span><span class="s1">np</span><span class="s3">.</span><span class="s1">zeros</span><span class="s3">((</span><span class="s1">nS </span><span class="s3">* </span><span class="s1">nA</span><span class="s3">, </span><span class="s1">nS</span><span class="s3">), </span><span class="s1">dtype</span><span class="s3">=</span><span class="s1">float</span><span class="s3">)</span>
<span class="s1">r </span><span class="s3">= </span><span class="s1">np</span><span class="s3">.</span><span class="s1">zeros</span><span class="s3">(</span><span class="s1">nS </span><span class="s3">* </span><span class="s1">nA</span><span class="s3">, </span><span class="s1">dtype</span><span class="s3">=</span><span class="s1">float</span><span class="s3">)</span>

<span class="s2">for </span><span class="s1">s </span><span class="s2">in </span><span class="s1">range</span><span class="s3">(</span><span class="s1">nS</span><span class="s3">):</span>
<span class="s0"># This will give // and %</span>
    <span class="s1">r0</span><span class="s3">, </span><span class="s1">c0 </span><span class="s3">= </span><span class="s1">divmod</span><span class="s3">(</span><span class="s1">s</span><span class="s3">, </span><span class="s1">ncol</span><span class="s3">)</span>

    <span class="s2">for </span><span class="s1">a </span><span class="s2">in </span><span class="s1">range</span><span class="s3">(</span><span class="s1">nA</span><span class="s3">):</span>
        <span class="s1">_s_id </span><span class="s3">= </span><span class="s1">sa_id</span><span class="s3">(</span><span class="s1">s</span><span class="s3">, </span><span class="s1">a</span><span class="s3">)</span>

        <span class="s0"># Goal is absorbing with reward 0</span>
        <span class="s2">if </span><span class="s1">s </span><span class="s3">== </span><span class="s1">s_goal</span><span class="s3">:</span>
            <span class="s1">P</span><span class="s3">[</span><span class="s1">_s_id</span><span class="s3">, </span><span class="s1">s_goal</span><span class="s3">] = </span><span class="s4">1.0</span>
            <span class="s1">r</span><span class="s3">[</span><span class="s1">_s_id</span><span class="s3">] = </span><span class="s4">0</span>
            <span class="s2">continue</span>

        <span class="s1">dr</span><span class="s3">, </span><span class="s1">dc </span><span class="s3">= </span><span class="s1">DELTAS</span><span class="s3">[</span><span class="s1">a</span><span class="s3">]</span>
        <span class="s1">rr </span><span class="s3">= </span><span class="s1">min</span><span class="s3">(</span><span class="s1">nrow </span><span class="s3">- </span><span class="s4">1</span><span class="s3">, </span><span class="s1">max</span><span class="s3">(</span><span class="s4">0</span><span class="s3">, </span><span class="s1">r0 </span><span class="s3">+ </span><span class="s1">dr</span><span class="s3">))</span>
        <span class="s1">cc </span><span class="s3">= </span><span class="s1">min</span><span class="s3">(</span><span class="s1">ncol </span><span class="s3">- </span><span class="s4">1</span><span class="s3">, </span><span class="s1">max</span><span class="s3">(</span><span class="s4">0</span><span class="s3">, </span><span class="s1">c0 </span><span class="s3">+ </span><span class="s1">dc</span><span class="s3">))</span>
        <span class="s1">s_next </span><span class="s3">= </span><span class="s1">s_id</span><span class="s3">(</span><span class="s1">rr</span><span class="s3">, </span><span class="s1">cc</span><span class="s3">)</span>

        <span class="s0"># Deterministic transition</span>
        <span class="s1">P</span><span class="s3">[</span><span class="s1">_s_id</span><span class="s3">, </span><span class="s1">s_next</span><span class="s3">] = </span><span class="s4">1.0</span>

        <span class="s0"># Reward: -1 per step, 0 in goal (already handled above)</span>
        <span class="s1">r</span><span class="s3">[</span><span class="s1">_s_id</span><span class="s3">] = -</span><span class="s4">1</span>
<hr class="ls0"><span class="s0">#%% md 
</span><span class="s1">### 4.2 Matrix Form of Bellman Consistency and Bellman equation 
 
Q-evaluation when a fixed policy $\pi$ is given: 
 
$$ 
Q_\pi(x,a) = r(x,a) + \gamma \,\mathbb{E}_{x'\sim P(\cdot \mid x,a)} \, V_\pi(x') \tag{4.3(1)} 
$$ 
 
The bellman equation: 
$$ 
Q^\star(x,a) \;=\; r(x,a) \;+\; \gamma \, \mathbb{E}_{x' \sim P(\cdot \mid x,a)} 
\left\{ \max_{a' \in A} Q^\star(x',a') \right\}, 
\qquad \forall (x,a) \in X \times A. \tag{4.3(2)} 
$$ 
 
where $Q^\star$ is the optimal $Q$-value function. Similarly, let us define 
 
$$ 
J_Q(x) = \max_{a \in A} Q(x,a). 
$$ 
 
Question: How to write these equations (4.3(1))&amp;(2) in matrix and operator form? <hr class="ls0"></span><span class="s0">#%% md 
</span><span class="s1">**(TODO) Answer:** 
 <hr class="ls0"></span><span class="s0">#%% md 
</span><span class="s1">--- 
**Answer: 4.2** 
 
Starting point: 
 
$$ 
Q_\pi(x,a) = r(x,a) + \gamma ,\mathbb{E}{x’\sim P(\cdot \mid x,a)} , V\pi(x’), 
\qquad V_\pi(x) = \sum_a \pi(a\mid x), Q_\pi(x,a). 
$$ 
 
Stacked into vector–matrix form: 
 
$$ 
Q_\pi = r + \gamma , P , V_\pi, 
\qquad V_\pi = \Pi Q_\pi, 
$$ 
 
where 
- $Q_\pi \in \mathbb{R}^{|X||A|}$ 
- $\in \mathbb{R}^{|X||A|}$ 
- $\in \mathbb{R}^{|X||A| \times |X|}$ 
- $i \in \mathbb{R}^{|X| \times |X||A|} with \Pi[x,(x,a)]=\pi(a\mid x)$ 
 
Eliminating $V_\pi$: 
 
$$ 
Q_\pi = r + \gamma , (P \Pi), Q_\pi. 
$$ 
 
So equivalently: 
 
$$ 
(\mathbf{I} - \gamma P \Pi), Q_\pi = r, 
\qquad 
Q_\pi = (\mathbf{I} - \gamma P \Pi)^{-1} r. 
$$ 
 
In state-value form: 
 
$$ 
V_\pi = R^\pi + \gamma P^\pi V_\pi, 
\qquad 
R^\pi := \Pi r, \quad P^\pi := \Pi P, 
$$ 
 
so 
 
$$ 
V_\pi = (\mathbf{I} - \gamma P^\pi)^{-1} R^\pi, 
\qquad 
Q_\pi = r + \gamma P V_\pi. 
$$ 
 
Define the policy Bellman operator: 
 
$$ 
(T^\pi Q) = r + \gamma, P, (\Pi Q). 
$$ 
 
Then $Q_\pi$ is the unique fixed point: 
 
$$ 
Q_\pi = T^\pi Q_\pi. 
$$ 
 
From the optimality condition: 
 
$$ 
Q^\star(x,a) = r(x,a) + \gamma ,\mathbb{E}{x’ \sim P(\cdot \mid x,a)} \Big[ \max{a’} Q^\star(x’,a’) \Big]. 
$$ 
 
Define the nonlinear operator: 
 
$$ 
(T^\star Q) = r + \gamma, P, J_Q, 
\qquad 
(J_Q)(x) := \max_{a} Q(x,a). 
$$ 
 
Then the optimal $Q^\star$ is the unique fixed point: 
 
$$ 
Q^\star = T^\star(Q^\star). 
$$ 
 
And the optimal state-value and greedy policy follow: 
 
$$ 
V^\star(x) = \max_a Q^\star(x,a), 
\qquad 
\pi^\star(x) \in \arg\max_a Q^\star(x,a). 
$$ 
 
--- <hr class="ls0"></span><span class="s0">#%% md 
</span><span class="s1">### 4.3 Solve bellman equation. 
 
Note that $J_Q$ has dimension $|X|$. With these notations, the *Bellman optimality operator* is defined as 
 
$$ 
T^\star Q \;=\; g + \gamma P J_Q, 
\tag{2.27} 
$$ 
 
which is nothing but a matrix representation of the right-hand side of Bellman equation. 
This allows us to concisely write the Bellman equation as 
 
$$ 
Q = T^\star Q. 
\tag{2.28} 
$$ 
 
One can do to solve this equation is through *fix-point iteration*: 
$$ 
Q_{n+1} = T^\star Q_n. 
$$ <hr class="ls0"></span><span class="s0">#%% 
</span><span class="s1">Q </span><span class="s3">= </span><span class="s1">np</span><span class="s3">.</span><span class="s1">zeros</span><span class="s3">((</span><span class="s1">nS </span><span class="s3">* </span><span class="s1">nA</span><span class="s3">), </span><span class="s1">dtype</span><span class="s3">=</span><span class="s1">float</span><span class="s3">)</span>
<span class="s2">for </span><span class="s1">i </span><span class="s2">in </span><span class="s1">range</span><span class="s3">(</span><span class="s4">1000</span><span class="s3">):</span>
    <span class="s1">old_Q </span><span class="s3">= </span><span class="s1">Q</span><span class="s3">.</span><span class="s1">copy</span><span class="s3">()</span>
    <span class="s1">J_Q </span><span class="s3">= </span><span class="s1">Q</span><span class="s3">.</span><span class="s1">reshape</span><span class="s3">(</span><span class="s1">nS</span><span class="s3">, </span><span class="s1">nA</span><span class="s3">).</span><span class="s1">max</span><span class="s3">(</span><span class="s1">axis</span><span class="s3">=</span><span class="s4">1</span><span class="s3">)</span>
    <span class="s1">Q </span><span class="s3">= </span><span class="s1">r </span><span class="s3">+ </span><span class="s1">gamma </span><span class="s3">* (</span><span class="s1">P </span><span class="s3">@ </span><span class="s1">J_Q</span><span class="s3">)</span>
    <span class="s2">if </span><span class="s1">np</span><span class="s3">.</span><span class="s1">max</span><span class="s3">(</span><span class="s1">np</span><span class="s3">.</span><span class="s1">abs</span><span class="s3">(</span><span class="s1">Q </span><span class="s3">- </span><span class="s1">old_Q</span><span class="s3">)) &lt; </span><span class="s4">1e-10</span><span class="s3">:</span>
        <span class="s1">print</span><span class="s3">(</span><span class="s5">f&quot;Converged in </span><span class="s2">{</span><span class="s1">i</span><span class="s3">+</span><span class="s4">1</span><span class="s2">} </span><span class="s5">iterations.&quot;</span><span class="s3">)</span>
        <span class="s2">break</span>
<span class="s1">J_Q </span><span class="s3">= </span><span class="s1">Q</span><span class="s3">.</span><span class="s1">reshape</span><span class="s3">(</span><span class="s1">nS</span><span class="s3">, </span><span class="s1">nA</span><span class="s3">).</span><span class="s1">max</span><span class="s3">(</span><span class="s1">axis</span><span class="s3">=</span><span class="s4">1</span><span class="s3">)    </span>
<span class="s1">print</span><span class="s3">(</span><span class="s5">&quot;</span><span class="s2">\n</span><span class="s5">Optimal state values J_Q (V*) on the grid:&quot;</span><span class="s3">)</span>
<span class="s2">for </span><span class="s1">r0 </span><span class="s2">in </span><span class="s1">range</span><span class="s3">(</span><span class="s1">nrow</span><span class="s3">):</span>
    <span class="s1">row_vals </span><span class="s3">= []</span>
    <span class="s2">for </span><span class="s1">c0 </span><span class="s2">in </span><span class="s1">range</span><span class="s3">(</span><span class="s1">ncol</span><span class="s3">):</span>
        <span class="s1">s </span><span class="s3">= </span><span class="s1">r0 </span><span class="s3">* </span><span class="s1">ncol </span><span class="s3">+ </span><span class="s1">c0</span>
        <span class="s2">if </span><span class="s1">s </span><span class="s3">== </span><span class="s1">s_start</span><span class="s3">:</span>
            <span class="s1">row_vals</span><span class="s3">.</span><span class="s1">append</span><span class="s3">(</span><span class="s5">&quot; S &quot;</span><span class="s3">)</span>
        <span class="s2">elif </span><span class="s1">s </span><span class="s3">== </span><span class="s1">s_goal</span><span class="s3">:</span>
            <span class="s1">row_vals</span><span class="s3">.</span><span class="s1">append</span><span class="s3">(</span><span class="s5">&quot; G &quot;</span><span class="s3">)</span>
        <span class="s2">else</span><span class="s3">:</span>
            <span class="s1">row_vals</span><span class="s3">.</span><span class="s1">append</span><span class="s3">(</span><span class="s5">f&quot;</span><span class="s2">{</span><span class="s1">J_Q</span><span class="s3">[</span><span class="s1">s</span><span class="s3">]</span><span class="s2">:</span><span class="s5">6.2f</span><span class="s2">}</span><span class="s5">&quot;</span><span class="s3">)</span>
    <span class="s1">print</span><span class="s3">(</span><span class="s5">&quot; &quot;</span><span class="s3">.</span><span class="s1">join</span><span class="s3">(</span><span class="s1">row_vals</span><span class="s3">))</span></pre>
</body>
</html>